Index: download_ich_csvs.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/bin/bash\n\n# ==============================\n# Usage: ./get_selected_scan.sh XNAT_SESSION_ID\n# ==============================\n\n# 1\uFE0F⃣ Take session ID from command line\nSESSION_ID=\"$1\"\nOUTPATH=\"/workingoutput\"\n\n# 2\uFE0F⃣ Call Python function to get selected scan (returns one string)\nRAW=$(\npython3 - <<EOF\nfrom download_with_session_ID import find_selected_scan_id\nprint(find_selected_scan_id(\"${SESSION_ID}\"))\nEOF\n)\necho $RAW\n\n# 3\uFE0F⃣ Parse \"SCAN_ID\"::<id>::\"SCAN_NAME\"::<name>\n#SCAN_ID=$(awk -F'::' '{gsub(/\"/,\"\",$2); print $2}' <<< \"$RAW\")\n#SCAN_NAME=$(awk -F'::' '{gsub(/^\"/,\"\",$4); gsub(/\"$/,\"\",$4); print $4}' <<< \"$RAW\")\n\n# 3\uFE0F⃣ Parse \"SCAN_ID\"::<id>::\"SCAN_NAME\"::<name>\nSCAN_ID=$(awk -F'::' '{gsub(/\"/,\"\",$2); print $2}' <<< \"$RAW\" | tr -d '[:space:]')\nSCAN_NAME=$(awk -F'::' '{gsub(/^\"/,\"\",$4); gsub(/\"$/,\"\",$4); print $4}' <<< \"$RAW\" | tr -d '[:space:]')\n\necho \"Selected scan ID:   $SCAN_ID\"\necho \"Selected scan name: $SCAN_NAME\"\n\n\necho \"Selected scan ID:   $SCAN_ID\"\necho \"Selected scan name: $SCAN_NAME\"\n\n# 4\uFE0F⃣ Export variables so Python can read them safely via environment\nexport SESSION_ID SCAN_ID OUTPATH\n\n# 5\uFE0F⃣ Run Python to get largest & newest CSV from ICH_PHE_QUANTIFICATION\npython3 - <<'PY'\nimport os, json\nfrom download_with_session_ID import get_largest_newest_csv_for_scan, download_xnat_file_to_path\n\nsession_id = os.environ[\"SESSION_ID\"]\nscan_id = os.environ[\"SCAN_ID\"]\nout_path = os.environ[\"OUTPATH\"]\n\ninfo = get_largest_newest_csv_for_scan(session_id, scan_id)\ndownload_xnat_file_to_path(info[\"uri\"], out_path)\n\nprint(json.dumps({\n    \"saved\": out_path,\n    \"name\": info[\"name\"],\n    \"size\": info[\"size\"],\n    \"created\": str(info[\"created\"])\n}, indent=2, ensure_ascii=False))\nPY\n##########################\n\n\n##!/bin/bash\n#\n## ==============================\n## Usage: ./get_selected_scan.sh XNAT_SESSION_ID\n## ==============================\n#\n## 1. Take session ID from command line\n#SESSION_ID=\"$1\"\n#\n## 2. Run the Python function and capture its output\n##SESSION_ID=\"$1\"\n#\n## 1) Call the Python function (returns one string)\n#RAW=$(\n#python3 - <<EOF\n#from download_with_session_ID import find_selected_scan_id\n#print(find_selected_scan_id(\"${SESSION_ID}\"))\n#EOF\n#)\n#\n## 2) Parse the string -> SCAN_ID and SCAN_NAME\n##    Format: \"SCAN_ID\"::<id>::\"SCAN_NAME\"::<name>\n#SCAN_ID=$(awk -F'::' '{gsub(/\"/,\"\",$2); print $2}' <<< \"$RAW\")\n#SCAN_NAME=$(awk -F'::' '{gsub(/^\"/,\"\",$4); gsub(/\"$/,\"\",$4); print $4}' <<< \"$RAW\")\n#\n## 3) Use them\n#echo \"Selected scan ID:   $SCAN_ID\"\n#echo \"Selected scan name: $SCAN_NAME\"\n#\n## 4. Print nicely or use them downstream\n#echo \"Selected scan ID:   $SCAN_ID\"\n#echo \"Selected scan name: $SCAN_NAME\"\n#\n##!/bin/bash\n## save as: fetch_best_csv.sh\n## Usage: ./fetch_best_csv.sh XNAT_SESSION_ID SCAN_ID /path/to/save.csv\n#\n## SESSION_ID=\"$1\"\n## SCAN_ID=\"$2\"\n#OUTPATH=\"/workingoutput\"\n#\n## 2. Run Python to get largest & newest CSV from ICH_PHE_QUANTIFICATION\n#python3 - <<EOF\n#import json\n#from download_with_session_ID import get_largest_newest_csv_for_scan, download_xnat_file_to_path\n#\n#session_id, scan_id, out_path = \"${SESSION_ID}\", \"${SCAN_ID}\", \"${OUTPATH}\"\n#info = get_largest_newest_csv_for_scan(session_id, scan_id)\n#download_xnat_file_to_path(info[\"uri\"], out_path)\n#print(json.dumps({\n#    \"saved\": out_path,\n#    \"name\": info[\"name\"],\n#    \"size\": info[\"size\"],\n#    \"created\": str(info[\"created\"])\n#}, ensure_ascii=False, indent=2))\n#EOF\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/download_ich_csvs.sh b/download_ich_csvs.sh
--- a/download_ich_csvs.sh	(revision 51d7187cf4437c7f93b9ba35f801fcadb7f1d19c)
+++ b/download_ich_csvs.sh	(date 1761254532320)
@@ -1,115 +1,68 @@
 #!/bin/bash
 
-# ==============================
-# Usage: ./get_selected_scan.sh XNAT_SESSION_ID
-# ==============================
-
-# 1️⃣ Take session ID from command line
-SESSION_ID="$1"
-OUTPATH="/workingoutput"
+SESSION_LABEL="IBIO_0066_10182020_1848_3"
 
-# 2️⃣ Call Python function to get selected scan (returns one string)
-RAW=$(
-python3 - <<EOF
-from download_with_session_ID import find_selected_scan_id
-print(find_selected_scan_id("${SESSION_ID}"))
+SESSION_ID=$(python3 - <<EOF
+from download_with_session_ID import get_session_id_from_label
+print(get_session_id_from_label("${SESSION_LABEL}"))
 EOF
 )
-echo $RAW
-
-# 3️⃣ Parse "SCAN_ID"::<id>::"SCAN_NAME"::<name>
-#SCAN_ID=$(awk -F'::' '{gsub(/"/,"",$2); print $2}' <<< "$RAW")
-#SCAN_NAME=$(awk -F'::' '{gsub(/^"/,"",$4); gsub(/"$/,"",$4); print $4}' <<< "$RAW")
-
-# 3️⃣ Parse "SCAN_ID"::<id>::"SCAN_NAME"::<name>
-SCAN_ID=$(awk -F'::' '{gsub(/"/,"",$2); print $2}' <<< "$RAW" | tr -d '[:space:]')
-SCAN_NAME=$(awk -F'::' '{gsub(/^"/,"",$4); gsub(/"$/,"",$4); print $4}' <<< "$RAW" | tr -d '[:space:]')
-
-echo "Selected scan ID:   $SCAN_ID"
-echo "Selected scan name: $SCAN_NAME"
-
-
-echo "Selected scan ID:   $SCAN_ID"
-echo "Selected scan name: $SCAN_NAME"
-
-# 4️⃣ Export variables so Python can read them safely via environment
-export SESSION_ID SCAN_ID OUTPATH
-
-# 5️⃣ Run Python to get largest & newest CSV from ICH_PHE_QUANTIFICATION
-python3 - <<'PY'
-import os, json
-from download_with_session_ID import get_largest_newest_csv_for_scan, download_xnat_file_to_path
-
-session_id = os.environ["SESSION_ID"]
-scan_id = os.environ["SCAN_ID"]
-out_path = os.environ["OUTPATH"]
-
-info = get_largest_newest_csv_for_scan(session_id, scan_id)
-download_xnat_file_to_path(info["uri"], out_path)
+echo "Session ID: $SESSION_ID"
 
-print(json.dumps({
-    "saved": out_path,
-    "name": info["name"],
-    "size": info["size"],
-    "created": str(info["created"])
-}, indent=2, ensure_ascii=False))
-PY
-##########################
-
-
-##!/bin/bash
 #
 ## ==============================
 ## Usage: ./get_selected_scan.sh XNAT_SESSION_ID
 ## ==============================
 #
-## 1. Take session ID from command line
+## 1️⃣ Take session ID from command line
 #SESSION_ID="$1"
+#OUTPATH="/workingoutput"
 #
-## 2. Run the Python function and capture its output
-##SESSION_ID="$1"
-#
-## 1) Call the Python function (returns one string)
+## 2️⃣ Call Python function to get selected scan (returns one string)
 #RAW=$(
 #python3 - <<EOF
 #from download_with_session_ID import find_selected_scan_id
 #print(find_selected_scan_id("${SESSION_ID}"))
 #EOF
 #)
+#echo $RAW
 #
-## 2) Parse the string -> SCAN_ID and SCAN_NAME
-##    Format: "SCAN_ID"::<id>::"SCAN_NAME"::<name>
-#SCAN_ID=$(awk -F'::' '{gsub(/"/,"",$2); print $2}' <<< "$RAW")
-#SCAN_NAME=$(awk -F'::' '{gsub(/^"/,"",$4); gsub(/"$/,"",$4); print $4}' <<< "$RAW")
+## 3️⃣ Parse "SCAN_ID"::<id>::"SCAN_NAME"::<name>
+##SCAN_ID=$(awk -F'::' '{gsub(/"/,"",$2); print $2}' <<< "$RAW")
+##SCAN_NAME=$(awk -F'::' '{gsub(/^"/,"",$4); gsub(/"$/,"",$4); print $4}' <<< "$RAW")
 #
-## 3) Use them
+## 3️⃣ Parse "SCAN_ID"::<id>::"SCAN_NAME"::<name>
+#SCAN_ID=$(awk -F'::' '{gsub(/"/,"",$2); print $2}' <<< "$RAW" | tr -d '[:space:]')
+#SCAN_NAME=$(awk -F'::' '{gsub(/^"/,"",$4); gsub(/"$/,"",$4); print $4}' <<< "$RAW" | tr -d '[:space:]')
+#
 #echo "Selected scan ID:   $SCAN_ID"
 #echo "Selected scan name: $SCAN_NAME"
 #
-## 4. Print nicely or use them downstream
+#
 #echo "Selected scan ID:   $SCAN_ID"
 #echo "Selected scan name: $SCAN_NAME"
 #
-##!/bin/bash
-## save as: fetch_best_csv.sh
-## Usage: ./fetch_best_csv.sh XNAT_SESSION_ID SCAN_ID /path/to/save.csv
+## 4️⃣ Export variables so Python can read them safely via environment
+#export SESSION_ID SCAN_ID OUTPATH
 #
-## SESSION_ID="$1"
-## SCAN_ID="$2"
-#OUTPATH="/workingoutput"
-#
-## 2. Run Python to get largest & newest CSV from ICH_PHE_QUANTIFICATION
-#python3 - <<EOF
-#import json
+## 5️⃣ Run Python to get largest & newest CSV from ICH_PHE_QUANTIFICATION
+#python3 - <<'PY'
+#import os, json
 #from download_with_session_ID import get_largest_newest_csv_for_scan, download_xnat_file_to_path
 #
-#session_id, scan_id, out_path = "${SESSION_ID}", "${SCAN_ID}", "${OUTPATH}"
+#session_id = os.environ["SESSION_ID"]
+#scan_id = os.environ["SCAN_ID"]
+#out_path = os.environ["OUTPATH"]
+#
 #info = get_largest_newest_csv_for_scan(session_id, scan_id)
 #download_xnat_file_to_path(info["uri"], out_path)
+#
 #print(json.dumps({
 #    "saved": out_path,
 #    "name": info["name"],
 #    "size": info["size"],
 #    "created": str(info["created"])
-#}, ensure_ascii=False, indent=2))
-#EOF
+#}, indent=2, ensure_ascii=False))
+#PY
+###########################
+#
Index: download_with_session_ID.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/python\n\nimport os, sys, errno, shutil, uuid,subprocess,csv,json\nimport math,inspect\nimport glob\nimport re,time\nimport requests\nimport pandas as pd\nimport nibabel as nib\nimport numpy as np\n# import pydicom as dicom\nimport pathlib\nimport argparse,xmltodict\nfrom xnatSession import XnatSession\n# from biomarker_db_module import BiomarkerDB\nfrom biomarkerdbclass import  BiomarkerDB\nfrom redcapapi_functions import *\ncatalogXmlRegex = re.compile(r'.*\\.xml$')\nXNAT_HOST_URL=os.environ['XNAT_HOST']  #'http://snipr02.nrg.wustl.edu:8080' #'https://snipr02.nrg.wustl.edu' #'https://snipr.wustl.edu'\nXNAT_HOST = XNAT_HOST_URL # os.environ['XNAT_HOST'] #\nXNAT_USER = os.environ['XNAT_USER']#\nXNAT_PASS =os.environ['XNAT_PASS'] #\napi_token=os.environ['REDCAP_API']\nxnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\nxnatSession.renew_httpsession()\nclass arguments:\n    def __init__(self,stuff=[]):\n        self.stuff=stuff\ndef call_fill_google_mysql_db_with_single_value(args):\n    db_table_name=args.stuff[1]\n    session_id=args.stuff[2]\n    column_name=args.stuff[3]\n    column_value=args.stuff[4]\n    try:\n        fill_google_mysql_db_with_single_value(db_table_name, session_id,column_name,column_value)\n    except:\n        pass\n\ndef call_fill_google_mysql_db_from_csv(args):\n    db_table_name=args.stuff[1]\n    csv_file_path=args.stuff[2]\n    id_column=args.stuff[3]\n    # column_value=args.stuff[4]\n    try:\n        fill_google_mysql_db_from_csv(db_table_name, csv_file_path, id_column=id_column)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n        subprocess.call(command,shell=True)\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n        subprocess.call(command,shell=True)\n        pass\n\ndef fill_google_mysql_db_from_csv(db_table_name, csv_file_path, id_column=\"session_id\"):\n      # Replace with actual module path\n    command = f\"echo  I am at : {db_table_name}::{csv_file_path}::{id_column}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n    subprocess.call(command,shell=True)\n    try:\n        df = pd.read_csv(csv_file_path)\n        command = f\"echo  I am at 1 : {db_table_name}::{csv_file_path}::{id_column}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n        subprocess.call(command,shell=True)\n    except Exception as e:\n        # print(f\"Failed to load CSV: {e}\")\n        command = f\"echo  failed at : {e}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n        subprocess.call(command,shell=True)\n        return\n    db = BiomarkerDB(\n        host=os.environ[\"GOOGLE_MYSQL_DB_IP\"],\n        user=\"root\",\n        password=os.environ[\"GOOGLE_MYSQL_DB_PASS\"],\n        database=\"BIOMARKERS\"\n    )\n\n\n    if not db.initialized:\n        print(\"Database initialization failed.\")\n        command = f\"echo  failed at : Database initialization failed.::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n        subprocess.call(command,shell=True)\n        return\n    command = f\"echo  I am  at : {id_column}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n    subprocess.call(command,shell=True)\n    for index, row in df.iterrows():\n        session_id = row[id_column]\n\n        for column_name in df.columns:\n            if column_name == id_column:\n                continue  # Skip ID column\n            column_value = row[column_name]\n            try:\n                db.upsert_single_field_by_id(db_table_name, session_id, column_name, column_value)\n                command = f\"echo  I am  at : {id_column}::{session_id}::{db_table_name}::{session_id}::{column_name}::{column_value}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n                subprocess.call(command,shell=True)\n            except Exception as e:\n                print(f\"Failed to insert {column_name}={column_value} for session_id={session_id}: {e}\")\n                command = f\"echo  failed at : {column_name}::{column_value}::{e}::\" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error1.txt\"\n                subprocess.call(command,shell=True)\n\n    db.close()\n\n\ndef fill_google_mysql_db_with_single_value(db_table_name, session_id,column_name,column_value):\n    db = BiomarkerDB(\n        host=os.environ[\"GOOGLE_MYSQL_DB_IP\"],             # Replace with your instance IP\n        user=\"root\",                     # Replace if using a different user\n        password=os.environ[\"GOOGLE_MYSQL_DB_PASS\"] , ##\"dharlabwustl1!\",   # Replace with your actual password\n        database=\"BIOMARKERS\"\n    )\n    try:\n        if db.initialized:\n            db.upsert_single_field_by_id(db_table_name, session_id, column_name, column_value)\n            db.close()\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n\ndef pipeline_step_completed(db_table_name,session_id,scan_id,column_name,column_value,resource_dir,list_of_file_ext_tobe_checked):\n    try:\n        # list_of_file_ext_tobe_checked_len=len(list_of_file_ext_tobe_checked)\n        count=0\n        URI=f'/data/experiments/{session_id}/scans/{scan_id}'\n        list_of_files_in_resource_dir=get_resourcefiles_metadata(URI,resource_dir)\n        df_scan_resource = pd.read_json(json.dumps(list_of_files_in_resource_dir)) #pd.read_json(json.dumps(metadata_masks))\n        # Check if each extension exists in **any** row in the url column\n        ext_found = [any(df_scan_resource['URI'].str.contains(ext, case=False)) for ext in list_of_file_ext_tobe_checked]\n\n        # Final decision: are all extensions found at least once in any row?\n        all_found = int(all(ext_found))\n        if all_found==1:\n            column_value=1 #all_found\n        # pd.DataFrame(df_scan).to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n        fill_google_mysql_db_with_single_value(db_table_name, session_id,column_name,column_value)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n    return 1\ndef call_pipeline_step_completed(args):\n    db_table_name=args.stuff[1]\n    session_id=args.stuff[2]\n    scan_id=args.stuff[3]\n    column_name=args.stuff[4]\n    column_value=args.stuff[5]\n    resource_dir=args.stuff[6]\n    list_of_file_ext_tobe_checked=args.stuff[7:]\n    try:\n        pipeline_step_completed(db_table_name,session_id,scan_id,column_name,column_value,resource_dir,list_of_file_ext_tobe_checked)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n    return 1\n\ndef get_scan_id_given_session_id_N_niftiname(session_id,niftiname):\n    this_session_metadata=get_metadata_session(session_id)\n    this_session_metadata_df = pd.read_json(json.dumps(this_session_metadata))\n    this_scan_id=''\n    for session_each_metadata_id, session_each_metadata in this_session_metadata_df.iterrows():\n        URL='/data/experiments/'+session_id+'/scans/'+str(session_each_metadata['ID'])\n        metadata_nifti=get_resourcefiles_metadata(URL,'NIFTI')\n        df_scan = pd.read_json(json.dumps(metadata_nifti))\n        for df_scan_each_id, df_scan_each in df_scan.iterrows():\n            if niftiname in str(df_scan_each['Name']): #.split('.ni')[0]==niftiname:\n                this_scan_id=str(session_each_metadata['ID'])\n    return this_scan_id\ndef get_scan_quality(session_id,scan_id,scan_assessor_name):\n    try:\n        url = (\"/data/experiments/%s/scans/%s/assessors?format=json\" %    (session_id,scan_id))\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        #xnatSession.close_httpsession())\n        print(get_field_from_nested_dict(response.json(), scan_assessor_name))\n        return get_field_from_nested_dict(response.json(), scan_assessor_name)\n    except Exception as e:\n        print(e)\n    return None\ndef get_field_from_nested_dict(data, target_field):\n    try:\n        if isinstance(data, dict):\n            # Check if the target field is in the dictionary\n            if target_field in data:\n                return data[target_field]\n            # Recursively search in the dictionary\n            for key, value in data.items():\n                result = get_field_from_nested_dict(value, target_field)\n                if result is not None:\n                    return result\n        elif isinstance(data, list):\n            # Iterate through the list and search in each element\n            for item in data:\n                result = get_field_from_nested_dict(item, target_field)\n                if result is not None:\n                    return result\n    except Exception as e:\n        print(e)\n        pass\n    return None\ndef change_type_of_scan(sessionId, scanId,label):\n    returnvalue=0\n    try:\n\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        url = (\"/data/experiments/%s/scans/%s?xsiType=xnat:ctScanData&type=%s\" % (sessionId, scanId, label))\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.put(xnatSession.host + url)\n        url = (\"/data/experiments/%s/scans/%s?xsiType=xnat:ctScanData&quality=%s\" % (sessionId, scanId, 'usable'))\n        #xnatSession.renew_httpsession()\n        response1 = xnatSession.httpsess.put(xnatSession.host + url)\n        if response.status_code == 200 or response1.status_code == 200:\n            print(\"Successfully set type for %s scan %s to '%s'\" % (sessionId, scanId, label))\n            print(\"Successfully set usability for %s scan %s to '%s'\" % (sessionId, scanId, 'usable'))\n            command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n            subprocess.call(command,shell=True)\n            returnvalue=1\n        # else:\n        #     errStr = \"ERROR\"\n        #     if response.status_code == 403 or response.status_code == 404:\n        #         errStr = \"PERMISSION DENIED\"\n        #     raise Exception(\"%s attempting to set series_class for %s %s to '%s': %s\" %\n        #                     (errStr, sessionId, scanId, label, response.text))\n    except Exception:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n    return  returnvalue\n\n\ndef call_change_type_of_scan(args):\n    returnvalue=0\n    try:\n        sessionId=args.stuff[1]\n        scanId=args.stuff[2]\n        label=args.stuff[3]\n        change_type_of_scan(sessionId, scanId,label)\n    except Exception:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n    return  returnvalue\ndef merge_csvs(csvfileslist,columntomatchlist,outputfilename):\n    df1=pd.read_csv(csvfileslist[0])\n    left_on=columntomatchlist[0]\n    for x in range(len(csvfileslist)):\n        if x > 0:\n            df2=pd.read_csv(csvfileslist[x])\n            df_cd = pd.merge(df1, df2, how='inner', left_on = 'Id', right_on = 'Id')\n\n            df2.rename(columns={csvfileslist[x]:left_on}, inplace=True)\n            df1 = df1.merge(df2, left_on = left_on, right_on = columntomatchlist[x])\n            left_on=columntomatchlist[x]\n\ndef combinecsvs_general(inputdirectory,outputdirectory,outputfilename,extension):\n    outputfilepath=os.path.join(outputdirectory,outputfilename)\n    extension = 'csv'\n    # pdffilesuffix='.csv'\n    # pdffiledirectory=inputdirectory\n    # all_filenames = get_latest_filesequence(pdffilesuffix,pdffiledirectory)\n    all_filenames = [i for i in glob.glob(os.path.join(inputdirectory,'*{}'.format(extension)))]\n#    os.chdir(inputdirectory)\n    #combine all files in the list\n    combined_csv=pd.read_csv(all_filenames[0])\n\n    for x in all_filenames:\n        try:\n            x_df=pd.read_csv(x)\n\n            # print(x_df.shape)\n            combined_csv=pd.concat([combined_csv,pd.read_csv(x)])\n        except:\n            pass\n\n    # combined_csv = pd.concat([pd.read_csv(all_filenames[0]),pd.read_csv(all_filenames[0])],axis=0) ##[pd.read_csv(f) for f in all_filenames ],axis=0)\n    combined_csv = combined_csv.drop_duplicates()\n    # combined_csv['FileName_slice'].replace('', np.nan, inplace=True)\n    # combined_csv.dropna(subset=['FileName_slice'], inplace=True)\n    #export to csv\n    combined_csv.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\n    # combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n    # combined_csv = combined_csv.drop_duplicates()\n    # combined_csv['FileName_slice'].replace('', np.nan, inplace=True)\n    # combined_csv.dropna(subset=['FileName_slice'], inplace=True)\n    # #export to csv\n    # combined_csv.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\ndef combinecsvs(inputdirectory,outputdirectory,outputfilename,extension):\n    outputfilepath=os.path.join(outputdirectory,outputfilename)\n    extension = 'csv'\n    pdffilesuffix='.csv'\n    pdffiledirectory=inputdirectory\n    all_filenames = get_latest_filesequence(pdffilesuffix,pdffiledirectory)\n\n    # print([f for f in all_filenames ])\n    # all_filenames = [i for i in glob.glob(os.path.join(inputdirectory,'*{}'.format(extension)))]\n    # print(all_filenames)\n    #    os.chdir(inputdirectory)\n    #combine all files in the list\n    combined_csv=pd.read_csv(all_filenames[0])\n\n    for x in all_filenames:\n        try:\n            x_df=pd.read_csv(x)\n\n            # print(x_df.shape)\n            combined_csv=pd.concat([combined_csv,pd.read_csv(x)])\n        except:\n            pass\n\n    # combined_csv = pd.concat([pd.read_csv(all_filenames[0]),pd.read_csv(all_filenames[0])],axis=0) ##[pd.read_csv(f) for f in all_filenames ],axis=0)\n    combined_csv = combined_csv.drop_duplicates()\n    combined_csv['FileName_slice'].replace('', np.nan, inplace=True)\n    combined_csv.dropna(subset=['FileName_slice'], inplace=True)\n    #export to csv\n    combined_csv.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\ndef combinecsvs_inafileoflist(listofcsvfiles_filename,outputdirectory,outputfilename):\n    try:\n        listofcsvfiles_filename_df=pd.read_csv(listofcsvfiles_filename)\n        listofcsvfiles_filename_df_list=list(listofcsvfiles_filename_df['LOCAL_FILENAME'])\n        outputfilepath=os.path.join(outputdirectory,outputfilename)\n        all_filenames = [i for i in listofcsvfiles_filename_df_list]\n        combined_csv=pd.read_csv(all_filenames[0])\n        for x in all_filenames:\n            try:\n                combined_csv=pd.concat([combined_csv,pd.read_csv(x)])\n            except:\n                pass\n        combined_csv = combined_csv.drop_duplicates()\n        combined_csv.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\n\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n        return 0\ndef call_combinecsvs_inafileoflist(args):\n    try:\n        listofcsvfiles_filename=args.stuff[1]\n        outputdirectory=args.stuff[2]\n        outputfilename=args.stuff[3]\n        combinecsvs_inafileoflist(listofcsvfiles_filename,outputdirectory,outputfilename)\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n        return 0\ndef combinecsvs_withprefix(inputdirectory,outputdirectory,outputfilename,prefix):\n    outputfilepath=os.path.join(outputdirectory,outputfilename)\n    extension = 'csv'\n    all_filenames = [i for i in glob.glob(os.path.join(inputdirectory,'{}*.{}'.format(prefix,extension)))]\n    #    os.chdir(inputdirectory)\n    #combine all files in the list\n    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n    combined_csv = combined_csv.drop_duplicates()\n    #export to csv\n    combined_csv.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\n\ndef copy_latest_pdffile(pdffileprefix,pdffiledirectory,destinationdirectory):\n    pdffilesuffix='.pdf'\n    allfileswithprefix1=get_latest_filesequence(pdffilesuffix,pdffiledirectory) #glob.glob(os.path.join(pdffiledirectory,pdffileprefix+'*'))\n    if len(allfileswithprefix1)>0:\n        # allfileswithprefix=sorted(allfileswithprefix1, key=os.path.getmtime)\n        # filetocopy=allfileswithprefix[0]\n        for filetocopy in allfileswithprefix1:\n            command = 'cp ' + filetocopy +'  ' + destinationdirectory\n            subprocess.call(command,shell=True)\ndef get_latest_filesequence(pdffilesuffix,pdffiledirectory):\n    latest_file_list=[]\n    allfileswithprefix1=glob.glob(os.path.join(pdffiledirectory,'*'+pdffilesuffix))\n    allfileswithprefix1_df = pd.DataFrame(allfileswithprefix1)\n    allfileswithprefix1_df.columns=[\"FILENAME\"]\n    # print(allfileswithprefix1_df)\n    allfileswithprefix1_df=allfileswithprefix1_df[allfileswithprefix1_df.FILENAME.str.contains(\"_thresh\")]\n    allfileswithprefix1_df['DATE']=allfileswithprefix1_df['FILENAME']\n    allfileswithprefix1_df['PREFIX']=allfileswithprefix1_df['FILENAME']\n    # allfileswithprefix1_df[['FILENAME', 'EXT']] = allfileswithprefix1_df['FILENAME'].str.split('.pdf', 1, expand=True) _thres\n    allfileswithprefix1_df[['PREFIX', 'EXT']] = allfileswithprefix1_df['PREFIX'].str.split('_thresh', 1, expand=True)\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str[-14:-4]\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str.replace('_', '')\n    allfileswithprefix1_df[\"PREFIX\"]=allfileswithprefix1_df[\"PREFIX\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n    # print(allfileswithprefix1_df['PREFIX']) #[0])\n    # print(np.unique(allfileswithprefix1_df['PREFIX']).shape)\n    # print(allfileswithprefix1_df['PREFIX'].shape)\n    unique_session_name=np.unique(allfileswithprefix1_df['PREFIX'])\n    allfileswithprefix1_df['DATETIME'] =    allfileswithprefix1_df['DATE']\n    allfileswithprefix1_df['DATETIME'] = pd.to_datetime(allfileswithprefix1_df['DATETIME'], format='%m%d%Y', errors='coerce')\n    # print(allfileswithprefix1_df['DATETIME'])\n    # print(unique_session_name)\n    for x in range(unique_session_name.shape[0]):\n        # print(unique_session_name[x])\n        x_df=allfileswithprefix1_df.loc[allfileswithprefix1_df['PREFIX'] == unique_session_name[x]]\n        x_df = x_df.sort_values(by=['DATETIME'], ascending=False)\n        x_df=x_df.reset_index(drop=True)\n        # print(x_df)\n        # if len(allfileswithprefix1)>0:\n        #     allfileswithprefix=sorted(allfileswithprefix1, key=os.path.getmtime)\n        filetocopy=x_df['FILENAME'][0]\n        # print(len(x_df['DATE'][0]))\n        # if len(x_df['DATE'][0])==8:\n        latest_file_list.append(filetocopy)\n    #     # command = 'cp ' + filetocopy +'  ' + destinationdirectory\n    #     # subprocess.call(command,shell=True)\n    return latest_file_list\ndef call_copy_latest_csvfile():\n    pdffileprefix=sys.argv[1]\n    pdffiledirectory=sys.argv[2]\n    destinationdirectory=sys.argv[3]\n    try:\n        copy_latest_pdffile(pdffileprefix,pdffiledirectory,destinationdirectory)\n    except:\n        pass\ndef copy_latest_csvfile(pdffileprefix,pdffiledirectory,destinationdirectory):\n    allfileswithprefix1=glob.glob(os.path.join(pdffiledirectory,pdffileprefix+'*'))\n    if len(allfileswithprefix1)>0:\n        allfileswithprefix=sorted(allfileswithprefix1, key=os.path.getmtime)\n        filetocopy=allfileswithprefix[0]\n        command = 'cp ' + filetocopy +'  ' + destinationdirectory\n        subprocess.call(command,shell=True)\ndef call_copy_latest_pdffile():\n    pdffileprefix=sys.argv[1]\n    pdffiledirectory=sys.argv[2]\n    destinationdirectory=sys.argv[3]\n    try:\n        copy_latest_pdffile(pdffileprefix,pdffiledirectory,destinationdirectory)\n    except:\n        pass\n\ndef call_combine_all_csvfiles_of_edema_biomarker():\n    working_directory=sys.argv[1]\n    working_directory_tocombinecsv=sys.argv[2]\n    extension=sys.argv[3]\n    outputfilename=sys.argv[4]\n    combinecsvs(working_directory,working_directory_tocombinecsv,outputfilename,extension)\ndef call_combine_all_csvfiles_general():\n    working_directory=sys.argv[1]\n    working_directory_tocombinecsv=sys.argv[2]\n    extension=sys.argv[3]\n    outputfilename=sys.argv[4]\n    combinecsvs_general(working_directory,working_directory_tocombinecsv,outputfilename,extension)\ndef call_combine_all_csvfiles_of_edema_biomarker_withprefix():\n    working_directory=sys.argv[1]\n    working_directory_tocombinecsv=sys.argv[2]\n    prefix=sys.argv[3]\n    outputfilename=sys.argv[4]\n    combinecsvs_withprefix(working_directory,working_directory_tocombinecsv,outputfilename,prefix)\ndef call_get_all_selected_scan_in_a_project():\n    projectId=sys.argv[1]\n    working_directory=sys.argv[2]\n    get_all_selected_scan_in_a_project(projectId,working_directory)\n\n\ndef call_get_all_EDEMA_BIOMARKER_csvfiles_of_allselectedscan():\n    working_directory=sys.argv[1]\n    get_all_EDEMA_BIOMARKER_csvfiles_of_ascan(working_directory)\n\n\ndef call_get_all_BIOMARKER_csvfiles_of_allselectedscan():\n    working_directory=sys.argv[1]\n    resource_dir=sys.argv[2]\n    # get_all_EDEMA_BIOMARKER_csvfiles_of_ascan(working_directory)\n    get_all_BIOMARKER_csvfiles_of_ascan(working_directory,resource_dir)\ndef add_a_column(csvfile,columname,columnvalue):\n    aa = pd.read_csv(csvfile)\n    aa[columname] = columnvalue\n    aa.to_csv(csvfile,index=False)\ndef call_add_a_column():\n    csvfile=sys.argv[1]\n    columname=sys.argv[2]\n    columnvalue=sys.argv[3]\n    add_a_column(csvfile,columname,columnvalue)\n\ndef merge_files_with_col_name(file1,file2,colname1,colname2):\n    csvfile=sys.argv[1]\n    columname=sys.argv[2]\n    columnvalue=sys.argv[3]\n    add_a_column(csvfile,columname,columnvalue)\n\n\ndef get_all_EDEMA_BIOMARKER_csvfiles_of_ascan(dir_to_receive_the_data):\n    for each_csvfile in glob.glob(os.path.join(dir_to_receive_the_data,'SNIPR*.csv')):\n        try:\n            df_selected_scan=pd.read_csv(each_csvfile)\n            resource_dir='EDEMA_BIOMARKER'\n            # print(df_selected_scan)\n            for item_id1, each_selected_scan in df_selected_scan.iterrows():\n                scan_URI=each_selected_scan['URI'].split('/resources/')[0] #/data/experiments/SNIPR_E03516/scans/2/resources/110269/files/BJH_002_11112019_1930_2.nii\n                print(scan_URI)\n                metadata_EDEMA_BIOMARKERS=get_resourcefiles_metadata(scan_URI,resource_dir)\n                if len(metadata_EDEMA_BIOMARKERS) >0:\n                    metadata_EDEMA_BIOMARKERS_df=pd.DataFrame(metadata_EDEMA_BIOMARKERS)\n                    print(metadata_EDEMA_BIOMARKERS_df)\n                    for item_id, each_file_inEDEMA_BIOMARKERS in metadata_EDEMA_BIOMARKERS_df.iterrows():\n                        if '.csv' in each_file_inEDEMA_BIOMARKERS['URI']:\n                            print(\"YES IT IS CSV FILE FOR ANALYSIS\")\n                            downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n                        if '.pdf' in each_file_inEDEMA_BIOMARKERS['URI']:\n                            print(\"YES IT IS PDF FILE FOR VISUALIZATION\")\n                            downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n                            # break\n                    # break\n        except:\n            pass\n\n\ndef get_all_BIOMARKER_csvfiles_of_ascan(dir_to_receive_the_data,resource_dir):\n    for each_csvfile in glob.glob(os.path.join(dir_to_receive_the_data,'SNIPR*.csv')):\n        try:\n            df_selected_scan=pd.read_csv(each_csvfile)\n            # resource_dir='EDEMA_BIOMARKER'\n            # print(df_selected_scan)\n            for item_id1, each_selected_scan in df_selected_scan.iterrows():\n                scan_URI=each_selected_scan['URI'].split('/resources/')[0] #/data/experiments/SNIPR_E03516/scans/2/resources/110269/files/BJH_002_11112019_1930_2.nii\n                print(scan_URI)\n                metadata_EDEMA_BIOMARKERS=get_resourcefiles_metadata(scan_URI,resource_dir)\n                if len(metadata_EDEMA_BIOMARKERS) >0:\n                    metadata_EDEMA_BIOMARKERS_df=pd.DataFrame(metadata_EDEMA_BIOMARKERS)\n                    print(metadata_EDEMA_BIOMARKERS_df)\n                    for item_id, each_file_inEDEMA_BIOMARKERS in metadata_EDEMA_BIOMARKERS_df.iterrows():\n                        if '.csv' in each_file_inEDEMA_BIOMARKERS['URI']:\n                            print(\"YES IT IS CSV FILE FOR ANALYSIS\")\n                            downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n                        if '.pdf' in each_file_inEDEMA_BIOMARKERS['URI']:\n                            print(\"YES IT IS PDF FILE FOR VISUALIZATION\")\n                            downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n                            # break\n                    # break\n        except:\n            pass\n\n\n\n# def combine_all_csvfiles_of_edema_biomarker(projectId,dir_to_receive_the_data):\n\n#     ## for each csv file corresponding to the session\n#     for each_csvfile in glob.glob(os.path.join(dir_to_receive_the_data,'*.csv')):\n#         df_selected_scan=pd.read_csv(each_csvfile)\n#         resource_dir='EDEMA_BIOMARKER'\n#         for item_id1, each_selected_scan in df_selected_scan.iterrows():\n#             scan_URI=each_selected_scan['URI'].split('/resources/')[0] #/data/experiments/SNIPR_E03516/scans/2/resources/110269/files/BJH_002_11112019_1930_2.nii\n#             metadata_EDEMA_BIOMARKERS=get_resourcefiles_metadata(scan_URI,resource_dir)\n#             metadata_EDEMA_BIOMARKERS_df=pd.DataFrame(metadata_EDEMA_BIOMARKERS)\n#             for item_id, each_file_inEDEMA_BIOMARKERS in sessions_list_df.iterrows():\n#                 # print(each_file_inEDEMA_BIOMARKERS['URI'])\n#                 if '.csv' in each_file_inEDEMA_BIOMARKERS['URI']:\n#                     print(\"YES IT IS CSV FILE FOR ANALYSIS\")\n#                     downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n#                 if '.pdf' in each_file_inEDEMA_BIOMARKERS['URI']:\n#                     print(\"YES IT IS CSV FILE FOR ANALYSIS\")\n#                     downloadresourcefilewithuri_py(each_file_inEDEMA_BIOMARKERS,dir_to_receive_the_data)\n\n\n#     # get_resourcefiles_metadata(URI,resource_dir)\n#     ## download csv files from the EDEMA_BIOMARKER directory:\n\n\n#     ## combine all the csv files\n\n#     ## upload the combined csv files to the project directory level\n\n\ndef deleteafile(filename):\n    command=\"rm \" + filename\n    subprocess.call(command,shell=True)\ndef get_all_selected_scan_in_a_project(projectId,dir_to_receive_the_data):\n    sessions_list=get_allsessionlist_in_a_project(projectId)\n    sessions_list_df=pd.DataFrame(sessions_list)\n    for item_id, each_session in sessions_list_df.iterrows():\n        sessionId=each_session['ID']\n        output_csvfile=os.path.join(dir_to_receive_the_data,sessionId+'.csv')\n        try:\n            decision_which_nifti(sessionId,dir_to_receive_the_data,output_csvfile)\n        except:\n            pass\n\ndef get_allsessionlist_in_a_project(projectId):\n    # projectId=\"BJH\" #sys.argv[1]\n    url = (\"/data/projects/%s/experiments/?format=json\" %    (projectId))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    sessions_list=response.json()['ResultSet']['Result']\n\n    return sessions_list\ndef call_decision_which_nifti():\n    sessionId=sys.argv[1]\n    dir_to_receive_the_data=sys.argv[2]\n    output_csvfile=sys.argv[3]\n    decision_which_nifti(sessionId,dir_to_receive_the_data,output_csvfile)\n\ndef call_decision_which_nifti_multiplescans():\n    sessionId=sys.argv[1]\n    dir_to_receive_the_data=sys.argv[2]\n    output_csvfile=sys.argv[3]\n    decision_which_nifti_multiplescans(sessionId,dir_to_receive_the_data,output_csvfile)\ndef count_brainaxial_or_thin(sessionId):\n    numberof_thin_or_axialscans=[0,0]\n    try:\n\n        this_session_metadata=get_metadata_session(sessionId)\n        jsonStr = json.dumps(this_session_metadata)\n        # print(jsonStr)\n        df = pd.read_json(jsonStr)\n        try :\n            numberof_thin_or_axialscans[0]=numberof_thin_or_axialscans[0]+df['type'].value_counts()['Z-Axial-Brain']\n        except:\n            pass\n        try :\n            numberof_thin_or_axialscans[1]=numberof_thin_or_axialscans[1]+df['type'].value_counts()['Z-Brain-Thin']\n        except:\n            pass\n        # numberof_thin_or_axialscans=[df['type'].value_counts()['Z-Axial-Brain'] , df['type'].value_counts()['Z-Brain-Thin']]\n        return  numberof_thin_or_axialscans #str(df_1.iloc[0][metadata_field])\n    except Exception:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        print(\"Exception::{}\".format(Exception))\n        pass\n    return numberof_thin_or_axialscans\ndef count_niftifiles_insession(sessionId,dir_to_receive_the_data):\n    numberofniftifiles=[0,\"\"]\n    try:\n\n        this_session_metadata=get_metadata_session(sessionId)\n        jsonStr = json.dumps(this_session_metadata)\n        # print(jsonStr)\n        df = pd.read_json(jsonStr)\n        for item_id, each_axial in df.iterrows():\n            URI=each_axial['URI'] #args.stuff[1] #sys.argv[1]\n            resource_dir=\"NIFTI\" #args.stuff[2] #sys.argv[2]\n            output_csvfile=os.path.join(dir_to_receive_the_data,sessionId+URI.split(\"/\")[-1]+\".csv\") #args.stuff[4] #sys.argv[4]\n            get_resourcefiles_metadata_saveascsv(URI,resource_dir,dir_to_receive_the_data,output_csvfile)\n\n            try :\n                output_csvfile_df=pd.read_csv(output_csvfile)\n                for item_id1, each_axial1 in output_csvfile_df.iterrows():\n                    if \".nii\" in each_axial1['Name']:\n                        numberofniftifiles[0]=numberofniftifiles[0]+1\n                        numberofniftifiles[1]=\"_\".join(each_axial1['Name'].split(\"_\")[0:len(each_axial1['Name'].split(\"_\"))-1])\n\n            except:\n                pass\n            print(\"I PASSED AT ::{}\".format(inspect.stack()[0][3]))\n\n        #\n        # numberofniftifiles=df['type'].value_counts()['Z-Axial-Brain'] + df['type'].value_counts()['Z-Brain-Thin']\n        # print(\"numberofniftifiles::{}\".format(numberofniftifiles))\n        return  numberofniftifiles #str(df_1.iloc[0][metadata_field])\n    except Exception:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        print(\"Exception::{}\".format(Exception))\n        pass\n    return numberofniftifiles\ndef get_single_value_from_metadata_forascan(sessionId,scanId,metadata_field):\n    returnvalue=\"\"\n    try:\n\n        this_session_metadata=get_metadata_session(sessionId)\n        jsonStr = json.dumps(this_session_metadata)\n        # print(jsonStr)\n        df = pd.read_json(jsonStr)\n        df['ID']=df['ID'].apply(str)\n        # this_session_metadata_df_scanid=df1[df1['ID'] == str(scanId1)]\n        df_1=df.loc[(df['ID'] == str(scanId))]\n        df_1=df_1.reset_index()\n        print(\" I AM AT get_single_value_from_metadata_forascan\")\n        print(df.columns)\n        print(\"I SUCCEEDED AT ::{}\".format(inspect.stack()[0][3]))\n        return str(df_1.iloc[0][metadata_field])\n    except Exception:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        print(\"Exception::{}\".format(Exception))\n        pass\n    return returnvalue\ndef decision_which_nifti_multiplescans(sessionId,dir_to_receive_the_data=\"\",output_csvfile=\"\"):\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n\n    # # df = pd.read_csv(sessionId+'_scans.csv')\n    # sorted_df = df.sort_values(by=['type'], ascending=False)\n    # # sorted_df.to_csv('scan_sorted.csv', index=False)\n    df_axial=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable')] # | (df['type'] == 'Z-Brain-Thin')  & (df['quality'] == 'usable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_thin=df.loc[(df['type'] == 'Z-Brain-Thin')  & (df['quality'] != 'unusable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    # print(df_axial)\n    list_of_usables=[]\n    list_of_usables_withsize=[]\n    file_uploaded_flag=0\n    if len(df_axial)>0:\n        selectedFile=\"\"\n    # print(len(df_axial))\n    # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        for item_id, each_axial in df_axial.iterrows():\n            print(each_axial['URI'])\n            URI=each_axial['URI']\n            resource_dir='NIFTI'\n            nifti_metadata=json.dumps(get_resourcefiles_metadata(URI,resource_dir)) #get_niftifiles_metadata(each_axial['URI'] )) get_resourcefiles_metadata(URI,resource_dir)\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                print(each_nifti['URI'])\n                if '.nii' in each_nifti['Name'] or '.nii.gz' in each_nifti['Name']:\n                    # list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID']])\n                    x=[each_nifti['URI'],each_nifti['Name'],each_axial['ID']]\n                    print(\"X::{}\".format(x))\n                    # # pd.DataFrame(final_ct_file).T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n                    # # now=time.localtime()\n                    # # date_time = time.strftime(\"_%m_%d_%Y\",now)\n                    niftifile_location=os.path.join(dir_to_receive_the_data,each_nifti['Name'].split(\".nii\")[0]+\"_NIFTILOCATION.csv\")\n\n                    downloadniftiwithuri(x,dir_to_receive_the_data)\n                    number_slice=nifti_number_slice(os.path.join(dir_to_receive_the_data,x[1]))\n                    # final_ct_file=[[each_nifti['URI'],each_nifti['Name'],each_axial['ID'],number_slice]]\n                    list_of_usables_withsize=[]\n                    if number_slice >=20 and number_slice <=100:\n                        list_of_usables_withsize.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID'],number_slice])\n                        jsonStr = json.dumps(list_of_usables_withsize)\n                        # print(jsonStr)\n                        df = pd.read_json(jsonStr)\n                        df.columns=['URI','Name','ID','NUMBEROFSLICES']\n                        df.to_csv(niftifile_location,index=False)\n                        resource_dirname=\"NIFTI_LOCATION\"\n                        url = ((\"/data/experiments/%s\") % (sessionId))\n                        uploadsinglefile_with_URI(url,niftifile_location,resource_dirname)\n                        file_uploaded_flag=1\n    if len(df_thin)>0 and file_uploaded_flag==0:\n        selectedFile=\"\"\n        # print(len(df_axial))\n        # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        df_axial=df_thin\n\n        for item_id, each_axial in df_axial.iterrows():\n            print(each_axial['URI'])\n            URI=each_axial['URI']\n            resource_dir='NIFTI'\n            nifti_metadata=json.dumps(get_resourcefiles_metadata(URI,resource_dir)) #get_niftifiles_metadata(each_axial['URI'] )) get_resourcefiles_metadata(URI,resource_dir)\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                print(each_nifti['URI'])\n                if '.nii' in each_nifti['Name'] or '.nii.gz' in each_nifti['Name']:\n                    # list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID']])\n                    x=[each_nifti['URI'],each_nifti['Name'],each_axial['ID']]\n                    print(\"X::{}\".format(x))\n                    # # pd.DataFrame(final_ct_file).T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n                    # # now=time.localtime()\n                    # # date_time = time.strftime(\"_%m_%d_%Y\",now)\n                    niftifile_location=os.path.join(dir_to_receive_the_data,each_nifti['Name'].split(\".nii\")[0]+\"_NIFTILOCATION.csv\")\n\n                    downloadniftiwithuri(x,dir_to_receive_the_data)\n                    number_slice=nifti_number_slice(os.path.join(dir_to_receive_the_data,x[1]))\n                    # final_ct_file=[[each_nifti['URI'],each_nifti['Name'],each_axial['ID'],number_slice]]\n                    list_of_usables_withsize=[]\n                    if number_slice <=200:\n                        list_of_usables_withsize.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID'],number_slice])\n                        jsonStr = json.dumps(list_of_usables_withsize)\n                        # print(jsonStr)\n                        df = pd.read_json(jsonStr)\n                        df.columns=['URI','Name','ID','NUMBEROFSLICES']\n                        df.to_csv(niftifile_location,index=False)\n                        resource_dirname=\"NIFTI_LOCATION\"\n                        url = ((\"/data/experiments/%s\") % (sessionId))\n                        uploadsinglefile_with_URI(url,niftifile_location,resource_dirname)\n\n        return\ndef get_info_from_xml(xmlfile):\n    try:\n        # session_id=args.stuff[1]\n        # xmlfile=args.stuff[2]\n        subprocess.call(\"echo \" + \"I xmlfile AT ::{}  >> /workingoutput/error.txt\".format(xmlfile) ,shell=True )\n        with open(xmlfile, encoding=\"utf-8\") as fd:\n            xmlfile_dict = xmltodict.parse(fd.read())\n\n        project_name=''\n        subject_name=''\n        session_label=''\n        acquisition_site_xml=''\n        acquisition_datetime_xml=''\n        scanner_from_xml=''\n        body_part_xml=''\n        kvp_xml=''\n        try:\n            session_label=xmlfile_dict['xnat:CTSession']['@label']\n        except:\n            pass\n        try:\n            project_name=xmlfile_dict['xnat:CTSession']['@project']\n        except:\n            pass\n        try:\n            subject_name=str(xmlfile_dict['xnat:CTSession']['xnat:dcmPatientName'])\n        except:\n            pass\n        # Acquisition site\n        try:\n            acquisition_site_xml=xmlfile_dict['xnat:CTSession']['xnat:acquisition_site']\n        except:\n            pass\n        try:\n            date_split=str(xmlfile_dict['xnat:CTSession']['xnat:date']).split('-')\n            columnvalue_1=\"/\".join([date_split[1],date_split[2],date_split[0]])\n            columnvalue_2=\":\".join(str(xmlfile_dict['xnat:CTSession']['xnat:time']).split(':')[0:2])\n            acquisition_datetime_xml=columnvalue_1+\" \"+ columnvalue_2\n        except:\n            pass\n        columnvalue_1=\"\"\n        columnvalue_2=\"\"\n        try:\n            for xx in range(len(xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'])):\n                if xx > 1:\n                    try:\n                        columnvalue_1= xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'][xx]['xnat:scanner']['@manufacturer']\n                        if len(columnvalue_1)>1:\n                            break\n                    except:\n                        pass\n            for xx in range(len(xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'])):\n                if xx > 1:\n                    try:\n                        columnvalue_2=  xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'][xx]['xnat:scanner']['@model']\n                        if len(columnvalue_2)>1:\n                            break\n                    except:\n                        pass\n\n            scanner_from_xml= columnvalue_1 + \" \" + columnvalue_2\n        except:\n            pass\n        ################\n        try:\n            for xx in range(len(xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'])):\n                if xx>1:\n                    try:\n                        body_part_xml=xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'][xx]['xnat:bodyPartExamined']\n                        # fill_datapoint_each_sessionn_1(identifier,columnname,columnvalue,csvfilename)\n                        if len(body_part_xml)>3:\n                            break\n                    except:\n                        pass\n\n        except:\n            pass\n        ###############\n        ###############\n\n        try:\n            for xx in range(len(xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'])):\n                if xx>1:\n                    try:\n                        kvp_xml=xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'][xx]['xnat:parameters']['xnat:kvp']\n                        # fill_datapoint_each_sessionn_1(identifier,columnname,columnvalue,csvfilename)\n                        if len(kvp_xml)>1:\n                            break\n                    except:\n                        pass\n            # columnvalue=xmlfile_dict['xnat:CTSession']['xnat:scans']['xnat:scan'][0]['xnat:parameters']['xnat:kvp']\n            # fill_datapoint_each_sessionn_1(identifier,columnname,columnvalue,csvfilename)\n        except:\n            subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n            pass\n        ###############\n\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT project_name::{}  >> /workingoutput/error.txt\".format(project_name) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT subject_name::{}  >> /workingoutput/error.txt\".format(subject_name) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT session_label::{}  >> /workingoutput/error.txt\".format(session_label) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT acquisition_site_xml::{}  >> /workingoutput/error.txt\".format(acquisition_site_xml) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT acquisition_datetime_xml::{}  >> /workingoutput/error.txt\".format(acquisition_datetime_xml) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT scanner_from_xml::{}  >> /workingoutput/error.txt\".format(scanner_from_xml) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT body_part_xml::{}  >> /workingoutput/error.txt\".format(body_part_xml) ,shell=True )\n        subprocess.call(\"echo \" + \"I PASSED AT kvp_xml::{}  >> /workingoutput/error.txt\".format(kvp_xml) ,shell=True )\n    except:\n        # print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        pass\n    return   project_name,subject_name, session_label,acquisition_site_xml,acquisition_datetime_xml,scanner_from_xml,body_part_xml,kvp_xml\ndef fill_single_val_redcap(args):\n    xmlfile=args.stuff[1]\n    csvfilename=args.stuff[2]\n    project_name,subject_name, session_label,acquisition_site_xml,acquisition_datetime_xml,scanner_from_xml,body_part_xml,kvp_xml=get_info_from_xml(xmlfile)\n    this_project_redcapfile_latest=project_name+'_latest.csv'\n    # api_token='EC6A2206FF8C1D87D4035E61C99290FF'\n    df_scan_latest=download_latest_redcapfile(api_token,this_project_redcapfile_latest)\n    this_session_redcap_repeat_instance_df=df_scan_latest[df_scan_latest['snipr_session']==session_label]\n    this_session_redcap_repeat_instance=str(this_session_redcap_repeat_instance_df['redcap_repeat_instance'])\n    add_one_data_to_redcap(subject_name,'imaging_data',this_session_redcap_repeat_instance,each_colname,csv_file_df[each_colname])\n    return\n\ndef find_num_axial_thin(args):\n    sessionId=args.stuff[1]\n    csvfilename=args.stuff[2]\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n    df_axial=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable')] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_axial_num=0\n    if df_axial.shape[0]>0:\n        df_axial_num=df_axial.shape[0]\n    df_thin=df.loc[(df['type'] == 'Z-Brain-Thin')  & (df['quality'] != 'unusable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_axial_thin_num=0\n    if df_thin.shape[0]>0:\n        df_axial_thin_num=df_thin.shape[0]\n    list_values_df=pd.DataFrame([df_axial_num,df_axial_thin_num])\n    list_values_df=list_values_df.T\n    list_values_df.columns=['axial_number','axial_thin_number']\n    list_values_df.to_csv(csvfilename,index=False)\n    return\n\ndef select_scan_for_analysis(args):\n    sessionId=args.stuff[1]\n    csvfilename=args.stuff[2]\n    dir_to_receive_the_data=args.stuff[3]\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n    df['NUMBEROFSLICES']=0\n    df_axial=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable')] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_thin=df.loc[(df['type'] == 'Z-Brain-Thin')  & (df['quality'] != 'unusable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    try:\n        df_axial=df_axial.reset_index()\n        for each_id in range(df_axial.shape[0]):\n            subprocess.call(\"echo \" + \"I PASSED AT each_id::{}  >> /workingoutput/error.txt\".format(each_id) ,shell=True )\n            URI=df_axial.at[each_id,'URI']\n            resource_dir='DICOM'\n            scan_meta_data=get_resourcefiles_metadata(URI,resource_dir)\n            jsonStr_scan = json.dumps(scan_meta_data)\n            # print(jsonStr)\n            df_scan = pd.read_json(jsonStr_scan)\n            df_axial.at[each_id,'NUMBEROFSLICES']=df_scan.shape[0]\n    except:\n        pass\n    df_scan.to_csv('test_df_scan.csv')\n    try:\n\n        df_thin=df_thin.reset_index()\n        for each_id in range(df_thin.shape[0]):\n            subprocess.call(\"echo \" + \"I PASSED AT each_id::{}  >> /workingoutput/error.txt\".format(each_id) ,shell=True )\n            URI=df_thin.at[each_id,'URI']\n            resource_dir='DICOM'\n            scan_meta_data=get_resourcefiles_metadata(URI,resource_dir)\n            jsonStr_scan = json.dumps(scan_meta_data)\n            # print(jsonStr)\n            df_scan = pd.read_json(jsonStr_scan)\n            df_thin.at[each_id,'NUMBEROFSLICES']=df_scan.shape[0]\n    except:\n        pass\n\n    if df_axial.shape[0]>0:\n        df_maxes = df_axial[df_axial['NUMBEROFSLICES']==df_axial['NUMBEROFSLICES'].max()]\n    elif df_thin.shape[0]>0:\n        df_maxes = df_thin[df_thin['NUMBEROFSLICES']==df_thin['NUMBEROFSLICES'].max()]\n######################\n    # return df_maxes\n\n    final_ct_file=''\n    if df_maxes.shape[0]>0:\n        final_ct_file=df_maxes.iloc[:1]\n        for item_id, each_scan in df_maxes.iterrows():\n\n            if \"tilt\" in each_scan['Name']:\n                final_ct_file=each_scan\n                break\n    if final_ct_file.shape[0] >= 1:\n        final_ct_file_df= final_ct_file #pd.DataFrame(final_ct_file)\n        for row_id, row_item in final_ct_file.iterrows():\n            niftifile_location=os.path.join(dir_to_receive_the_data,row_item['Name'].split(\".nii\")[0]+\"_NIFTILOCATION.csv\")\n            final_ct_file_df.to_csv(niftifile_location,index=False)\n            resource_dirname=\"NIFTI_LOCATION\"\n            url = ((\"/data/experiments/%s\") % (sessionId))\n            uploadsinglefile_with_URI(url,niftifile_location,resource_dirname)\n\n#############################\n    df.to_csv(csvfilename,index=False)\n    return\ndef fill_redcap_for_selected_scan(args):\n    try:\n\n        # session_id=args.stuff[1]\n        # subprocess.call(\"echo \" + \"I zai zeli AT ::{}  >> /workingoutput/error.txt\".format(session_id) ,shell=True )\n        xmlfile=args.stuff[1]\n        csv_file_df=pd.read_csv(args.stuff[2])\n        # project_name,subject_name, session_label,acquisition_site_xml,acquisition_datetime_xml,scanner_from_xml,body_part_xml,kvp_xml\n        project_name,subject_name, session_label,acquisition_site_xml,acquisition_datetime_xml,scanner_from_xml,body_part_xml,kvp_xml=get_info_from_xml(xmlfile)\n        this_project_redcapfile_latest=project_name+'_latest.csv'\n        # api_token='EC6A2206FF8C1D87D4035E61C99290FF'\n        df_scan_latest=download_latest_redcapfile(api_token,this_project_redcapfile_latest)\n        this_session_redcap_repeat_instance_df=df_scan_latest[df_scan_latest['snipr_session']==session_label]\n        this_session_redcap_repeat_instance=str(this_session_redcap_repeat_instance_df['redcap_repeat_instance'].item())\n        imaging_data_complete=str(this_session_redcap_repeat_instance_df['imaging_data_complete'].item())\n        if imaging_data_complete != '2':\n            for each_colname in csv_file_df.columns:\n                # print(each_colname)\n                # print(csv_file_df[each_colname])\n                subprocess.call(\"echo \" + \"I PASSED AT subject_name::{}  >> /workingoutput/error.txt\".format(subject_name) ,shell=True )\n                subprocess.call(\"echo \" + \"I PASSED AT this_session_redcap_repeat_instance::{}  >> /workingoutput/error.txt\".format(this_session_redcap_repeat_instance) ,shell=True )\n                subprocess.call(\"echo \" + \"I PASSED AT session_label::{}  >> /workingoutput/error.txt\".format(session_label) ,shell=True )\n                subprocess.call(\"echo \" + \"I PASSED AT each_colname::{}  >> /workingoutput/error.txt\".format(each_colname) ,shell=True )\n                subprocess.call(\"echo \" + \"I PASSED AT each_colname_value::{}  >> /workingoutput/error.txt\".format(csv_file_df[each_colname][0]) ,shell=True )\n                try:\n                    add_one_data_to_redcap(subject_name,'imaging_data',this_session_redcap_repeat_instance,str(each_colname),csv_file_df[each_colname].item())\n                except:\n                    subprocess.call(\"echo \" + \"I FAILED AT subject_name::{}  >> /workingoutput/error.txt\".format(subject_name) ,shell=True )\n                    pass\n        #fill scan base\n        ## fill scan complete name\n        ## fill number of slices,kvp,px,pz, scanner detail\n\n        # subprocess.call(\"echo \" + \"I PASSED AT project_name::{}  >> /workingoutput/error.txt\".format(project_name) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT subject_name::{}  >> /workingoutput/error.txt\".format(subject_name) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT session_label::{}  >> /workingoutput/error.txt\".format(session_label) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT acquisition_site_xml::{}  >> /workingoutput/error.txt\".format(acquisition_site_xml) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT acquisition_datetime_xml::{}  >> /workingoutput/error.txt\".format(acquisition_datetime_xml) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT scanner_from_xml::{}  >> /workingoutput/error.txt\".format(scanner_from_xml) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT body_part_xml::{}  >> /workingoutput/error.txt\".format(body_part_xml) ,shell=True )\n        # subprocess.call(\"echo \" + \"I PASSED AT kvp_xml::{}  >> /workingoutput/error.txt\".format(kvp_xml) ,shell=True )\n    except:\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        pass\n    return\ndef fill_redcap_for_pdffile(args):\n    try:\n\n        # session_id=args.stuff[1]\n        # subprocess.call(\"echo \" + \"I zai zeli AT ::{}  >> /workingoutput/error.txt\".format(session_id) ,shell=True )\n        xmlfile=args.stuff[1]\n        file_name=args.stuff[2]\n        project_name,subject_name, session_label,acquisition_site_xml,acquisition_datetime_xml,scanner_from_xml,body_part_xml,kvp_xml=get_info_from_xml(xmlfile)\n        this_project_redcapfile_latest=project_name+'_latest.csv'\n        # api_token='EC6A2206FF8C1D87D4035E61C99290FF'\n        df_scan_latest=download_latest_redcapfile(api_token,this_project_redcapfile_latest)\n        this_session_redcap_repeat_instance_df=df_scan_latest[df_scan_latest['snipr_session']==session_label]\n        this_session_redcap_repeat_instance=str(this_session_redcap_repeat_instance_df['redcap_repeat_instance'].item())\n        imaging_data_complete=str(this_session_redcap_repeat_instance_df['imaging_data_complete'].item())\n        if imaging_data_complete != '2':\n            add_one_file_to_redcap(subject_name,'imaging_data',this_session_redcap_repeat_instance,str('session_pdf'),file_name)\n    except:\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        pass\n    return\ndef decision_which_nifti_used_untilAug62025(sessionId,dir_to_receive_the_data=\"\",output_csvfile=\"\"):\n    # sessionId=sys.argv[1]\n    # dir_to_receive_the_data=\"./NIFTIFILEDIR\" #sys.argv[2]\n    # output_csvfile='test.csv' #sys.argv[3]\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n    # # df = pd.read_csv(sessionId+'_scans.csv')\n    # sorted_df = df.sort_values(by=['type'], ascending=False)\n    # # sorted_df.to_csv('scan_sorted.csv', index=False)\n    df_axial_all=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable')] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_axial=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable')]\n    try:\n        if df_axial_all.shape[0]>0:\n            df_axial_all_num_usable = df_axial_all[df_axial_all['quality'] == 'usable' ].shape[0]\n            if df_axial_all_num_usable.shape[0]>0:\n                df_axial=df_axial_all_num_usable\n    except:\n        pass\n    df_thin_all=df.loc[(df['type'] == 'Z-Brain-Thin')  & (df['quality'] != 'unusable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_thin=df.loc[(df['type'] == 'Z-Brain-Thin')  & (df['quality'] != 'unusable') ]\n    try:\n        if df_thin_all.shape[0]>0:\n            df_thin_all_num_usable = df_thin_all[df_thin_all['quality'] == 'usable' ].shape[0]\n            if df_thin_all_num_usable.shape[0]>0:\n                df_thin=df_thin_all_num_usable\n    except:\n        pass\n    # print(df_axial)\n    list_of_usables=[]\n    list_of_usables_withsize=[]\n    if len(df_axial)>0:\n        selectedFile=\"\"\n        # print(len(df_axial))\n        # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        for item_id, each_axial in df_axial.iterrows():\n            print(each_axial['URI'])\n            URI=each_axial['URI']\n            resource_dir='NIFTI'\n            nifti_metadata=json.dumps(get_resourcefiles_metadata(URI,resource_dir)) #get_niftifiles_metadata(each_axial['URI'] )) get_resourcefiles_metadata(URI,resource_dir)\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                print(each_nifti['URI'])\n                if '.nii' in each_nifti['Name'] or '.nii.gz' in each_nifti['Name']:\n                    list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID']])\n                    x=[each_nifti['URI'],each_nifti['Name'],each_axial['ID']]\n                    downloadniftiwithuri(x,dir_to_receive_the_data)\n                    number_slice=nifti_number_slice(os.path.join(dir_to_receive_the_data,x[1]))\n                    if number_slice <=250 : # and number_slice <=100:\n                    # df_maxes=df[df.eval(\"NUMBEROFSLICES >=20 & (NUMBEROFSLICES <=70)\" )]\n                        list_of_usables_withsize.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID'],number_slice])\n                    # deleteafile(os.path.join(dir_to_receive_the_data,x[1]))\n\n\n\n            # break\n    elif len(df_thin)>0:\n        selectedFile=\"\"\n        # print(len(df_axial))\n        # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        for item_id, each_thin in df_thin.iterrows():\n            print(each_thin['URI'])\n            URI=each_thin['URI']\n            resource_dir='NIFTI'\n            nifti_metadata=json.dumps(get_resourcefiles_metadata(URI,resource_dir)) #json.dumps(get_niftifiles_metadata(each_thin['URI'] ))\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                # print(each_nifti['URI'])\n                if '.nii' in each_nifti['Name'] or '.nii.gz' in each_nifti['Name']:\n                    x=[each_nifti['URI'],each_nifti['Name'],each_thin['ID']]\n                    list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_thin['ID']])\n                    x=[each_nifti['URI'],each_nifti['Name'],each_thin['ID']]\n                    downloadniftiwithuri(x,dir_to_receive_the_data)\n                    number_slice=nifti_number_slice(os.path.join(dir_to_receive_the_data,x[1]))\n                    if  number_slice <=250:\n                        list_of_usables_withsize.append([each_nifti['URI'],each_nifti['Name'],each_thin['ID'],number_slice])\n                    # print(\"number_slice:{}\".format(number_slice))\n\n                    # deleteafile(os.path.join(dir_to_receive_the_data,x[1]))\n\n            # break\n    # dir_to_receive_the_data=\"./NIFTIFILEDIR\"\n    # final_ct_file=list_of_usables[0]\n    if len(list_of_usables_withsize) > 0:\n        jsonStr = json.dumps(list_of_usables_withsize)\n        # print(jsonStr)\n        df = pd.read_json(jsonStr)\n        df.columns=['URI','Name','ID','NUMBEROFSLICES']\n        # df_maxes = df[df['NUMBEROFSLICES']>=20 & df['NUMBEROFSLICES']<=65]\n        # df=df[df.eval(\"NUMBEROFSLICES >=20 & (NUMBEROFSLICES <=70)\" )]\n        # print(\"df_maxes::{}\".format(df_maxes))\n        df_maxes = df[df['NUMBEROFSLICES']==df['NUMBEROFSLICES'].max()]\n\n        # return df_maxes\n        final_ct_file=''\n        if df_maxes.shape[0]>0:\n            final_ct_file=df_maxes.iloc[0]\n            for item_id, each_scan in df_maxes.iterrows():\n                if \"tilt\" in each_scan['Name']:\n                    final_ct_file=each_scan\n                    break\n        if len(final_ct_file)> 1:\n            final_ct_file_df=pd.DataFrame(final_ct_file)\n            # pd.DataFrame(final_ct_file).T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n            final_ct_file_df.T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n            # now=time.localtime()\n            # date_time = time.strftime(\"_%m_%d_%Y\",now)\n            print(\"final_ct_file_df::{}\".format(final_ct_file_df.T))\n            for final_ct_file_df_item_id, final_ct_file_df_each_scan in final_ct_file_df.T.iterrows():\n                # if final_ct_file_df_each_scan['NUMBEROFSLICES'] >= 20 and final_ct_file_df_each_scan['NUMBEROFSLICES'] <= 65:\n\n                niftifile_location=os.path.join(dir_to_receive_the_data,final_ct_file_df_each_scan['Name'].split(\".nii\")[0]+\"_NIFTILOCATION.csv\")\n                # pd.DataFrame(final_ct_file)\n                final_ct_file_df.T.to_csv(niftifile_location,index=False)\n                ####################################################\n\n                # now=time.localtime()\n                # date_time = time.strftime(\"_%m_%d_%Y\",now)\n                # niftifile_location=os.path.join(\"/output\",\"NIFTIFILE_LOCATION\"+\"_\" +sessionId+\"_\" +scanId+date_time+\".csv\")\n                # df_listfile.to_csv(niftifile_location,index=False)\n\n                resource_dirname=\"NIFTI_LOCATION\"\n                url = ((\"/data/experiments/%s\") % (sessionId))\n                uploadsinglefile_with_URI(url,niftifile_location,resource_dirname)\n                print(\"final_ct_file_df::{}\".format(final_ct_file_df.T))\n\n            ########################################################\n                # try:\n                #     fill_redcap_for_selected_scan()\n                # except:\n                #     pass\n                return True\n        return False\n    else:\n        return False\n\n\ndef decision_which_nifti(sessionId, dir_to_receive_the_data=\"\", output_csvfile=\"\"):\n    def get_best_from_df(df, slice_order='max'):\n        candidates = []\n\n        for _, row in df.iterrows():\n            URI = row['URI']\n            scan_id = row['ID']\n            resource_dir = \"NIFTI\"\n            nifti_meta = json.dumps(get_resourcefiles_metadata(URI, resource_dir))\n            df_nifti = pd.read_json(nifti_meta)\n            for _, nifti in df_nifti.iterrows():\n                if \".nii\" in nifti[\"Name\"] or \".nii.gz\" in nifti[\"Name\"]:\n                    downloadniftiwithuri([nifti[\"URI\"], nifti[\"Name\"], scan_id], dir_to_receive_the_data)\n                    path = os.path.join(dir_to_receive_the_data, nifti[\"Name\"])\n                    num_slices = nifti_number_slice(path)\n                    candidates.append([nifti[\"URI\"], nifti[\"Name\"], scan_id, num_slices])\n        if not candidates:\n            return None\n        df_cand = pd.DataFrame(candidates, columns=[\"URI\", \"Name\", \"ID\", \"NUMBEROFSLICES\"])\n        if slice_order == 'max':\n            selected = df_cand[df_cand[\"NUMBEROFSLICES\"] == df_cand[\"NUMBEROFSLICES\"].max()]\n        else:\n            selected = df_cand[df_cand[\"NUMBEROFSLICES\"] == df_cand[\"NUMBEROFSLICES\"].min()]\n        return selected.iloc[0] if not selected.empty else None\n\n    # Load session metadata\n    this_session_metadata = get_metadata_session(sessionId)\n    df = pd.read_json(json.dumps(this_session_metadata))\n\n    # Categorize scans\n    axial_usable = df[(df['type'] == 'Z-Axial-Brain') & (df['quality'] == 'usable')]\n    axial_questionable = df[(df['type'] == 'Z-Axial-Brain') & (df['quality'] == 'questionable')]\n    thin_usable = df[(df['type'] == 'Z-Brain-Thin') & (df['quality'] == 'usable')]\n    thin_questionable = df[(df['type'] == 'Z-Brain-Thin') & (df['quality'] == 'questionable')]\n    # Collect counts\n    category_counts = {\n        \"axial_usable\": len(axial_usable),\n        \"axial_questionable\": len(axial_questionable),\n        \"thin_usable\": len(thin_usable),\n        \"thin_questionable\": len(thin_questionable),\n    }\n\n    # Convert to DataFrame\n    summary_df = pd.DataFrame(list(category_counts.items()), columns=[\"Category\", \"Count\"])\n\n    # Save to CSV\n    output_path = \"/output/this_session_metadata_count.csv\"\n    summary_df.to_csv(output_path, index=False)\n\n    # df.to_csv('/output/this_session_metadata_1.csv',index=False)\n    # Apply decision logic\n    selected_scan = None\n    if not axial_usable.empty:\n        selected_scan = get_best_from_df(axial_usable, slice_order='max')\n    elif  not thin_usable.empty:\n        if not axial_questionable.empty:\n            selected_scan = get_best_from_df(axial_questionable, slice_order='max')\n\n        else:\n            selected_scan = get_best_from_df(thin_usable, slice_order='min')\n    elif not axial_questionable.empty:\n        selected_scan = get_best_from_df(axial_questionable, slice_order='max')\n\n    elif not thin_questionable.empty:\n        selected_scan = get_best_from_df(thin_questionable, slice_order='min')\n\n    else:\n        print(\"No scan selected\")\n\n        return False\n\n    # Export and upload\n    if selected_scan is not None:\n        df_final = pd.DataFrame([selected_scan])\n        df_final.to_csv(os.path.join(dir_to_receive_the_data, output_csvfile), index=False)\n\n        niftifile_location = os.path.join(\n            dir_to_receive_the_data,\n            selected_scan[\"Name\"].split(\".nii\")[0] + \"_NIFTILOCATION.csv\"\n        )\n        df_final.to_csv(niftifile_location, index=False)\n\n        resource_dirname = \"NIFTI_LOCATION\"\n        url = f\"/data/experiments/{sessionId}\"\n        uploadsinglefile_with_URI(url, niftifile_location, resource_dirname)\n        return True\n\n    return False\n\ndef nifti_number_slice(niftifilename):\n    return nib.load(niftifilename).shape[2]\n\n    # pd.DataFrame(final_ct_file).T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n# def downloadniftiwithuri(URI,dir_to_save,niftioutput_filename):\n#     #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n#     # for x in range(df.shape[0]):\n#     #     print(df.iloc[x])\n\n#     url =   URI #  df.iloc[0][0] #URI_name[0] #(\"/data/experiments/%s/scans/%s/resources/NIFTI/files?format=zip\" %\n#         # (sessionId, scanId))\n#     print(url)\n#     #xnatSession.renew_httpsession()\n#     response = xnatSession.httpsess.get(xnatSession.host + url)\n#     zipfilename=os.path.join(dir_to_save,niftioutput_filename) #sessionId+scanId+'.zip'\n#     with open(zipfilename, \"wb\") as f:\n#         for chunk in response.iter_content(chunk_size=512):\n#             if chunk:  # filter out keep-alive new chunks\n#                 f.write(chunk)\n#     #xnatSession.close_httpsession())\n\n# def get_urls_csvfiles_in_EDEMA_BIOMARKER_inaproject(sessions_list):\n#     jsonStr = json.dumps(sessions_list)\n#     # print(jsonStr)\n#     df = pd.read_json(jsonStr)\n#     for item_id, each_session in df_touse.iterrows():\n#         sessionId=each_session['ID']\n#         this_session_metadata=get_metadata_session(sessionId)\n\n\n#     this_session_metadata=get_metadata_session(sessionId)\n#     jsonStr = json.dumps(this_session_metadata)\n#     # print(jsonStr)\n#     df = pd.read_json(jsonStr)\n#     df_touse=df.loc[(df['ID'] == int(scanId))]\n\n#     # print(\"get_resourcefiles_metadata(df_touse['URI'],resource_foldername ){}\".format(get_resourcefiles_metadata(df_touse['URI'],resource_foldername )))\n#     for item_id, each_scan in df_touse.iterrows():\n#         print(\"each_scan['URI'] {}\".format(each_scan['URI']))\n#         nifti_metadata=json.dumps(get_resourcefiles_metadata(each_scan['URI'],resource_foldername ))\n#         df_scan = pd.read_json(nifti_metadata)\n#         pd.DataFrame(df_scan).to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n\n\ndef get_maskfile_scan_metadata():\n    sessionId=sys.argv[1]\n    scanId=sys.argv[2]\n    resource_foldername=sys.argv[3]\n    dir_to_receive_the_data=sys.argv[4]\n    output_csvfile=sys.argv[5]\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n    df_touse=df.loc[(df['ID'] == int(scanId))]\n\n    # print(\"get_resourcefiles_metadata(df_touse['URI'],resource_foldername ){}\".format(get_resourcefiles_metadata(df_touse['URI'],resource_foldername )))\n    for item_id, each_scan in df_touse.iterrows():\n        print(\"each_scan['URI'] {}\".format(each_scan['URI']))\n        nifti_metadata=json.dumps(get_resourcefiles_metadata(each_scan['URI'],resource_foldername ))\n        df_scan = pd.read_json(nifti_metadata)\n        pd.DataFrame(df_scan).to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\ndef get_relevantfile_from_NIFTIDIR():\n    sessionId=sys.argv[1]\n    dir_to_receive_the_data=sys.argv[2]\n    output_csvfile=sys.argv[3]\n    this_session_metadata=get_metadata_session(sessionId)\n    jsonStr = json.dumps(this_session_metadata)\n    # print(jsonStr)\n    df = pd.read_json(jsonStr)\n    # # df = pd.read_csv(sessionId+'_scans.csv')\n    # sorted_df = df.sort_values(by=['type'], ascending=False)\n    # # sorted_df.to_csv('scan_sorted.csv', index=False)\n    df_axial=df.loc[(df['type'] == 'Z-Axial-Brain') & (df['quality'] != 'unusable') ] ##| (df['type'] == 'Z-Brain-Thin')]\n    df_thin=df.loc[(df['type'] == 'Z-Brain-Thin') & (df['quality'] != 'unusable')] ##| (df['type'] == 'Z-Brain-Thin')]\n    # print(df_axial)\n    list_of_usables=[]\n    if len(df_axial)>0:\n        selectedFile=\"\"\n        # print(len(df_axial))\n        # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        for item_id, each_axial in df_axial.iterrows():\n            print(each_axial['URI'])\n            nifti_metadata=json.dumps(get_niftifiles_metadata(each_axial['URI'] ))\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                print(each_nifti['URI'])\n                list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_axial['ID']])\n            break\n    elif len(df_thin)>0:\n        selectedFile=\"\"\n        # print(len(df_axial))\n        # print(\"df_axial:{}\".format(len(df_axial['URI'])))\n        for item_id, each_thin in df_thin.iterrows():\n            print(each_thin['URI'])\n            nifti_metadata=json.dumps(get_niftifiles_metadata(each_thin['URI'] ))\n            df_scan = pd.read_json(nifti_metadata)\n\n            for each_item_id,each_nifti in df_scan.iterrows():\n                print(each_nifti['URI'])\n                list_of_usables.append([each_nifti['URI'],each_nifti['Name'],each_thin['ID']])\n            break\n    final_ct_file=list_of_usables[0]\n    for x in list_of_usables:\n        if \"tilt\" in x[0].lower():\n            final_ct_file=x\n            break\n    # downloadniftiwithuri(final_ct_file,dir_to_receive_the_data)\n    pd.DataFrame(final_ct_file).T.to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n\ndef downloadniftiwithuri_withcsv():\n    csvfilename=sys.argv[1]\n    dir_to_save=sys.argv[2]\n    df=pd.read_csv(csvfilename)\n    print('csvfilename::{}::dir_to_save::{}'.format(csvfilename,dir_to_save))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    for x in range(df.shape[0]):\n        print(df.iloc[x])\n\n        url =     df.iloc[x][0] #URI_name[0] #(\"/data/experiments/%s/scans/%s/resources/NIFTI/files?format=zip\" %\n            # (sessionId, scanId))\n        print(url)\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        zipfilename=os.path.join(dir_to_save,df.iloc[x][1]) #sessionId+scanId+'.zip'\n        with open(zipfilename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=512):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        #xnatSession.close_httpsession())\n\ndef downloadmaskswithuri_withcsv():\n    csvfilename=sys.argv[1]\n    dir_to_save=sys.argv[2]\n    df=pd.read_csv(csvfilename)\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    for item_id, each_scan in df.iterrows():\n        # print(\"each_scan['URI'] {}\".format(each_scan['URI']))\n    # for x in range(df.shape[0]):\n        # print(df.iloc[x])\n\n        url =  each_scan['URI'] #   df.iloc[0][0] #URI_name[0] #(\"/data/experiments/%s/scans/%s/resources/NIFTI/files?format=zip\" %\n            # (sessionId, scanId))\n        print(url)\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        zipfilename=os.path.join(dir_to_save,each_scan['Name']) #sessionId+scanId+'.zip'\n        with open(zipfilename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=512):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        #xnatSession.close_httpsession())\ndef downloadresourcefilewithuri_py(url,dir_to_save):\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url['URI'])\n    zipfilename=os.path.join(dir_to_save,url['Name']) #sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    #xnatSession.close_httpsession())\ndef dowload_a_folder_as_zip(sessionId, scanId,resource_dir):\n    ##xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    try:\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        url = (\"/data/experiments/%s/scans/%s/resources/%s/files?format=zip\" %\n               (sessionId, scanId,resource_dir))\n\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        zipfilename=os.path.join('/workinginput',sessionId+scanId+resource_dir+'.zip')\n        with open(zipfilename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=512):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n        subprocess.call(command,shell=True)\n        # command='rm -r ' + zipfilename\n        # command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n        # subprocess.call(command,shell=True)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        return True\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\ndef call_dowload_a_folder_as_zip(args):\n    try:\n        sessionId=args.stuff[1]\n        scanId=args.stuff[2]\n        resource_dir=args.stuff[3]\n        dowload_a_folder_as_zip(sessionId, scanId,resource_dir)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\ndef downloadniftiwithuri(URI_name,dir_to_save):\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    url = URI_name[0] #(\"/data/experiments/%s/scans/%s/resources/NIFTI/files?format=zip\" %\n        # (sessionId, scanId))\n    print(url)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    zipfilename=os.path.join(dir_to_save,URI_name[1]) #sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    #xnatSession.close_httpsession())\n\ndef get_niftifiles_metadata(URI):\n    url = (URI+'/resources/NIFTI/files?format=json')\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_nifti=response.json()['ResultSet']['Result']\n    return metadata_nifti\ndef get_resourcefiles_metadata(URI,resource_dir):\n    url = (URI+'/resources/' + resource_dir +'/files?format=json')\n    print(url)\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_masks=response.json()['ResultSet']['Result']\n    return metadata_masks\ndef get_resourcefiles_metadata_saveascsv(URI,resource_dir,dir_to_receive_the_data,output_csvfile):\n\n    url = (URI+'/resources/' + resource_dir +'/files?format=json')\n    # print(\"url::{}\".format(url))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_masks=response.json()['ResultSet']['Result']\n    # print(\"metadata_masks::{}\".format(metadata_masks))\n    df_scan = pd.read_json(json.dumps(metadata_masks))\n    pd.DataFrame(df_scan).to_csv(os.path.join(dir_to_receive_the_data,output_csvfile),index=False)\n    # return metadata_masks\ndef call_get_resourcefiles_metadata_saveascsv():\n    try:\n        URI=sys.argv[1]\n        # print(\"URI::{}\".format(URI))\n        URI=URI.split('/resources')[0]\n        # print(\"URI::{}\".format(URI))\n        resource_dir=sys.argv[2]\n        dir_to_receive_the_data=sys.argv[3]\n        output_csvfile=sys.argv[4]\n        get_resourcefiles_metadata_saveascsv(URI,resource_dir,dir_to_receive_the_data,output_csvfile)\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n    return 0\n\ndef call_get_resourcefiles_metadata_saveascsv_args(args):\n    try:\n        URI=args.stuff[1] #sys.argv[1]\n        # print(\"URI::{}\".format(URI))\n        URI=URI.split('/resources')[0]\n        # print(\"URI::{}\".format(URI))\n        resource_dir=args.stuff[2] #sys.argv[2]\n        dir_to_receive_the_data=args.stuff[3] #sys.argv[3]\n        output_csvfile=args.stuff[4] #sys.argv[4]\n        get_resourcefiles_metadata_saveascsv(URI,resource_dir,dir_to_receive_the_data,output_csvfile)\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n    return 0\ndef findthetargetscan():\n     target_scan=\"\"\n     ## find the list of usable scans\n     ## Is axial available? If yes, focus on axial, if not go for thin\n     ## Is tilt available ? If yes, get the tilt, if not go for non-tilt\n     return target_scan\ndef downloadfile_withasuffix(sessionId,scanId,output_dirname,resource_dirname,file_suffix):\n    try:\n\n        # print('sessionId::scanId::resource_dirname::output_dirname::{}::{}::{}::{}'.format(sessionId,scanId,resource_dirname,output_dirname))\n        url = ((\"/data/experiments/%s/scans/%s/resources/\"+resource_dirname+\"/files/\") % (sessionId, scanId))\n        df_listfile=listoffile_witha_URI_as_df(url)\n        for item_id, row in df_listfile.iterrows():\n            if file_suffix in str(row['URI']) : ##.str.contains(file_suffix):\n                download_a_singlefile_with_URIString(row['URI'],row['Name'],output_dirname)\n                print(\"DOWNLOADED ::{}\".format(row))\n\n        return True\n    except Exception as exception:\n        print(\"FAILED AT ::{}\".format(exception))\n        pass\n    return  False\n\n\ndef uploadfile():\n    sessionId=str(sys.argv[1])\n    scanId=str(sys.argv[2])\n    input_dirname=str(sys.argv[3])\n    resource_dirname=str(sys.argv[4])\n    file_suffix=str(sys.argv[5])\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    url = ((\"/data/experiments/%s/scans/%s/resources/\"+resource_dirname+\"/files/\") % (sessionId, scanId))\n    allniftifiles=glob.glob(os.path.join(input_dirname,'*'+file_suffix) ) #input_dirname + '/*'+file_suffix)\n    for eachniftifile in allniftifiles:\n        files={'file':open(eachniftifile,'rb')}\n        response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n        print(response)\n    #xnatSession.close_httpsession())\n    # for eachniftifile in allniftifiles:\n    #     command= 'rm  ' + eachniftifile\n    #     subprocess.call(command,shell=True)\n    return True\n\ndef uploadfile_withprefix():\n    sessionId=str(sys.argv[1])\n    scanId=str(sys.argv[2])\n    input_dirname=str(sys.argv[3])\n    resource_dirname=str(sys.argv[4])\n    file_suffix=str(sys.argv[5])\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    url = ((\"/data/experiments/%s/scans/%s/resources/\"+resource_dirname+\"/files/\") % (sessionId, scanId))\n    allniftifiles=glob.glob(os.path.join(input_dirname,file_suffix+'*') ) #input_dirname + '/*'+file_suffix)\n    for eachniftifile in allniftifiles:\n        files={'file':open(eachniftifile,'rb')}\n        response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n        print(response)\n    #xnatSession.close_httpsession())\n    # for eachniftifile in allniftifiles:\n    #     command= 'rm  ' + eachniftifile\n    #     subprocess.call(command,shell=True)\n    return True\ndef uploadsinglefile():\n    sessionId=str(sys.argv[1])\n    scanId=str(sys.argv[2])\n    input_dirname=str(sys.argv[3])\n    resource_dirname=str(sys.argv[4])\n    file_name=str(sys.argv[5])\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    url = ((\"/data/experiments/%s/scans/%s/resources/\"+resource_dirname+\"/files/\") % (sessionId, scanId))\n    # allniftifiles=glob.glob(os.path.join(input_dirname,'*'+file_suffix) ) #input_dirname + '/*'+file_suffix)\n    # for eachniftifile in allniftifiles:\n    eachniftifile=os.path.join(input_dirname,file_name)\n    files={'file':open(eachniftifile,'rb')}\n    response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n    print(response)\n    #xnatSession.close_httpsession())\n    # for eachniftifile in allniftifiles:\n    #     command= 'rm  ' + eachniftifile\n    #     subprocess.call(command,shell=True)\n    return True\ndef uploadsinglefile_X_level(X_level,projectId,eachniftifile,resource_dirname):\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    url = ((\"/data/\"+X_level+\"/%s/resources/\"+resource_dirname+\"/files/\") % (projectId))\n    files={'file':open(eachniftifile,'rb')}\n    response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n    print(response)\n    #xnatSession.close_httpsession())\n\ndef uploadfile_projectlevel():\n    try:\n        projectId=str(sys.argv[1])\n        input_dirname=str(sys.argv[2])\n        resource_dirname=str(sys.argv[3])\n        file_suffix=str(sys.argv[4])\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        url = ((\"/data/projects/%s/resources/\"+resource_dirname+\"/files/\") % (projectId))\n        allniftifiles=glob.glob(os.path.join(input_dirname,'*'+file_suffix) ) #input_dirname + '/*'+file_suffix)\n        for eachniftifile in allniftifiles:\n            files={'file':open(eachniftifile,'rb')}\n            response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n            print(response)\n        #xnatSession.close_httpsession())\n        # for eachniftifile in allniftifiles:\n        #     command= 'rm  ' + eachniftifile\n        #     subprocess.call(command,shell=True)\n        return True\n    except Exception as e:\n        print(e)\n        return False\ndef call_uploadsinglefile_with_URI(args):\n    url=args.stuff[1]\n    file_name=args.stuff[2]\n    resource_dirname=args.stuff[3]\n    # url=args.stuff[1]\n    uploadsinglefile_with_URI(url,file_name,resource_dirname)\n\ndef uploadsinglefile_with_URI(url,file_name,resource_dirname):\n    try:\n\n        url = url+\"/resources/\"+resource_dirname+\"/files/\"\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        files={'file':open(file_name,'rb')}\n        response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n        print(\"response::{}\".format(response))\n\n        #xnatSession.close_httpsession())\n        print(\"I UPLOADED FILE WITH  uploadsinglefile_with_URI\")\n    except:\n        print(\"I FAILED AT uploadsinglefile_with_URI\")\n        pass\n\ndef uploadfilesfromlistinacsv(urllistfilename,X_level,projectId,resource_dirname):\n    try:\n        urllistfilename_df=pd.read_csv(urllistfilename)\n        for item_id, row in urllistfilename_df.iterrows():\n            eachniftifile=row['LOCAL_FILENAME']\n            # print(\"eachniftifile_ROW::{}\".format(row))\n            uploadsinglefile_X_level(X_level,projectId,eachniftifile,resource_dirname)\n            print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1 #print(\"ROW::{}\".format(row['LOCAL_FILENAME'])) #print(\"X_level::{}::projectId::{}::eachniftifile::{}::resource_dirname::{}\".format(X_level,projectId,eachniftifile,resource_dirname))\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n    return 0\n\ndef call_uploadfilesfromlistinacsv(args):\n    urllistfilename=args.stuff[1]\n    X_level=args.stuff[2]\n    projectId=args.stuff[3]\n    resource_dirname=args.stuff[4]\n    uploadfilesfromlistinacsv(urllistfilename,X_level,projectId,resource_dirname)\ndef uploadsinglefile_projectlevel_args(args):\n    try:\n        projectId=args.stuff[1] #str(sys.argv[1])\n        input_dirname=args.stuff[2] #str(sys.argv[2])\n        resource_dirname=args.stuff[3] #str(sys.argv[3])\n        file_name=args.stuff[4] # str(sys.argv[4])\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        url = ((\"/data/projects/%s/resources/\"+resource_dirname+\"/files/\") % (projectId))\n        # allniftifiles=glob.glob(os.path.join(input_dirname,'*'+file_suffix) ) #input_dirname + '/*'+file_suffix)\n        eachniftifile=os.path.join(input_dirname,file_name)\n        files={'file':open(eachniftifile,'rb')}\n        # for eachniftifile in allniftifiles:\n        #     files={'file':open(eachniftifile,'rb')}\n        response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n        print(response)\n        #xnatSession.close_httpsession())\n        # for eachniftifile in allniftifiles:\n        #     command= 'rm  ' + eachniftifile\n        #     subprocess.call(command,shell=True)\n        return True\n    except Exception as e:\n        print(e)\n        return False\ndef uploadsinglefile_projectlevel():\n    try:\n        projectId=str(sys.argv[1])\n        input_dirname=str(sys.argv[2])\n        resource_dirname=str(sys.argv[3])\n        file_name=str(sys.argv[4])\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        url = ((\"/data/projects/%s/resources/\"+resource_dirname+\"/files/\") % (projectId))\n        # allniftifiles=glob.glob(os.path.join(input_dirname,'*'+file_suffix) ) #input_dirname + '/*'+file_suffix)\n        eachniftifile=os.path.join(input_dirname,file_name)\n        files={'file':open(eachniftifile,'rb')}\n        # for eachniftifile in allniftifiles:\n        #     files={'file':open(eachniftifile,'rb')}\n        response = xnatSession.httpsess.post(xnatSession.host + url,files=files)\n        print(response)\n        #xnatSession.close_httpsession())\n        # for eachniftifile in allniftifiles:\n        #     command= 'rm  ' + eachniftifile\n        #     subprocess.call(command,shell=True)\n        return True\n    except Exception as e:\n        print(e)\n        return False\ndef downloadandcopyfile():\n    sessionId=sys.argv[1]\n    scanId=sys.argv[2]\n    metadata_session=get_metadata_session(sessionId)\n    decision=decide_image_conversion(metadata_session,scanId)\n    command= 'rm -r /ZIPFILEDIR/*'\n    subprocess.call(command,shell=True)\n    if decision==True:\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        outcome=get_nifti_using_xnat(sessionId, scanId)\n        if outcome==False:\n            print(\"NO DICOM FILE %s:%s:%s:%s\" % (sessionId, scanId))\n        #xnatSession.close_httpsession())\n        try :\n            copy_nifti()\n            print(\"COPIED TO WORKINGDIRECTORY\")\n        except:\n            pass\ndef downloadandcopyallniftifiles():\n    sessionId=sys.argv[1]\n    scanId=sys.argv[2]\n    metadata_session=get_metadata_session(sessionId)\n    # decision=decide_image_conversion(metadata_session,scanId)\n    command= 'rm -r /ZIPFILEDIR/*'\n    subprocess.call(command,shell=True)\n    # if decision==True:\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    outcome=get_nifti_using_xnat(sessionId, scanId)\n    if outcome==False:\n        print(\"NO DICOM FILE %s:%s:%s:%s\" % (sessionId, scanId))\n    #xnatSession.close_httpsession())\n    try :\n        copy_nifti()\n        print(\"COPIED TO WORKINGDIRECTORY\")\n    except:\n        pass\ndef copy_nifti():\n    for dirpath, dirnames, files in os.walk('/ZIPFILEDIR'):\n    #                print(f'Found directory: {dirpath}')\n        for file_name in files:\n            file_extension = pathlib.Path(file_name).suffix\n            if 'nii' in file_extension:\n                command='cp ' + os.path.join(dirpath,file_name) + '  /workinginput/'\n                subprocess.call(command,shell=True)\n                print(os.path.join(dirpath,file_name))\n\n\n\n\ndef get_slice_idx(nDicomFiles):\n    return min(nDicomFiles-1, math.ceil(nDicomFiles*0.7)) # slice 70% through the brain\ndef get_metadata_subject(project_id,subject_id,outputfile=\"NONE.csv\"):\n    url = (\"/data/projects/%s/subjects/%s/experiments/?format=json\" %    (project_id,subject_id))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_subj=response.json()['ResultSet']['Result']\n    metadata_subj_1=json.dumps(metadata_subj)\n    df_scan = pd.read_json(metadata_subj_1)\n    df_scan.to_csv(outputfile,index=False)\n    return metadata_subj\ndef get_metadata_session(sessionId,outputfile=\"NONE.csv\"):\n    url = (\"/data/experiments/%s/scans/?format=json\" %    (sessionId))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_session=response.json()['ResultSet']['Result']\n    metadata_session_1=json.dumps(metadata_session)\n    df_scan = pd.read_json(metadata_session_1)\n    df_scan.to_csv(outputfile,index=False)\n    return metadata_session\ndef get_metadata_project_sessionlist(project_ID,outputfile=\"NONE.csv\"):\n    url = (\"/data/projects/%s/experiments/?format=json\" %    (project_ID))\n    # /data/projects/${project_ID}/experiments/\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_session=response.json()['ResultSet']['Result']\n    metadata_session_1=json.dumps(metadata_session)\n    df_scan = pd.read_json(metadata_session_1)\n    df_scan.to_csv(outputfile,index=False)\n    return metadata_session\ndef get_session_label(sessionId,outputfile=\"NONE.csv\"):\n    returnvalue=''\n    try:\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        # sessionId='SNIPR02_E02933'\n\n        url = (\"/data/experiments/%s/?format=json\" %    (sessionId)) #scans/\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        #xnatSession.close_httpsession())\n        session_label=response.json()['items'][0]['data_fields']['label']\n        df_session=pd.DataFrame([session_label])\n        df_session.columns=['SESSION_LABEL']\n        df_session.to_csv(outputfile,index=False)\n        command=\"echo successful at :: {}::maskfilename::{} >> /software/error.txt\".format(inspect.stack()[0][3],'get_session_label')\n        subprocess.call(command,shell=True)\n        returnvalue=session_label\n    except:\n        command=\"echo failed at :: {} >> /software/error.txt\".format(inspect.stack()[0][3])\n        subprocess.call(command,shell=True)\n    return returnvalue\ndef get_session_project(sessionId,outputfile=\"NONE.csv\"):\n    returnvalue=''\n    try:\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        # sessionId='SNIPR02_E02933'\n\n        url = (\"/data/experiments/%s/?format=json\" %    (sessionId)) #scans/\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        #xnatSession.close_httpsession())\n        session_label=response.json()['items'][0]['data_fields']['project']\n        df_session=pd.DataFrame([session_label])\n        df_session.columns=['SESSION_PROJECT']\n        df_session.to_csv(outputfile,index=False)\n        command=\"echo successful at :: {}::maskfilename::{} >> /software/error.txt\".format(inspect.stack()[0][3],'get_session_project')\n        subprocess.call(command,shell=True)\n        returnvalue=session_label\n    except:\n        command=\"echo failed at :: {} >> /software/error.txt\".format(inspect.stack()[0][3])\n        subprocess.call(command,shell=True)\n    return returnvalue\n\n\ndef call_get_session_label(args):\n    returnvalue=0\n    try:\n        sessionId=args.stuff[1]\n        outputfile=args.stuff[2]\n        get_session_label(sessionId,outputfile=outputfile)\n        command=\"echo successful at :: {}::maskfilename::{} >> /software/error.txt\".format(inspect.stack()[0][3],'call_get_metadata_session')\n        subprocess.call(command,shell=True)\n        returnvalue=1\n    except:\n        command=\"echo failed at :: {} >> /software/error.txt\".format(inspect.stack()[0][3])\n        subprocess.call(command,shell=True)\n    return returnvalue\ndef call_get_session_project(args):\n    returnvalue=0\n    try:\n        sessionId=args.stuff[1]\n        outputfile=args.stuff[2]\n        get_session_project(sessionId,outputfile=outputfile)\n        command=\"echo successful at :: {}::maskfilename::{} >> /software/error.txt\".format(inspect.stack()[0][3],'call_get_session_project')\n        subprocess.call(command,shell=True)\n        returnvalue=1\n    except:\n        command=\"echo failed at :: {} >> /software/error.txt\".format(inspect.stack()[0][3])\n        subprocess.call(command,shell=True)\n    return returnvalue\n\n\ndef call_get_metadata_session(args):\n    returnvalue=0\n    try:\n        outpufilename=\"\"\n        sessionId=args.stuff[1]\n        try:\n            outpufilename=args.stuff[2]\n        except:\n            pass\n        if len(outpufilename)>0:\n            get_metadata_session(sessionId,outpufilename)\n        else:\n            get_metadata_session(sessionId)\n        command=\"echo successful at :: {}::maskfilename::{} >> /software/error.txt\".format(inspect.stack()[0][3],'call_get_metadata_session')\n        subprocess.call(command,shell=True)\n        returnvalue=1\n    except:\n        command=\"echo failed at :: {} >> /software/error.txt\".format(inspect.stack()[0][3])\n        subprocess.call(command,shell=True)\n    return returnvalue\ndef get_metadata_session_forbash():\n    sessionId=sys.argv[1]\n    url = (\"/data/experiments/%s/scans/?format=json\" %    (sessionId))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    #xnatSession.close_httpsession())\n    metadata_session=response.json()['ResultSet']['Result']\n    print(metadata_session)\n    data_file = open('this_sessionmetadata.csv', 'w')\n    csv_writer = csv.writer(data_file)\n    count = 0\n    for data in metadata_session:\n        if count == 0:\n            header = data.keys()\n            csv_writer.writerow(header)\n            count += 1\n        csv_writer.writerow(data.values())\n    data_file.close()\n    return metadata_session\ndef decide_image_conversion(metadata_session,scanId):\n    decision=False\n    usable=False\n    brain_type=False\n    for x in metadata_session:\n        if x['ID']  == scanId:\n            print(x['ID'])\n            # result_usability = response.json()['ResultSet']['Result'][0]['quality']\n            result_usability = x['quality']\n#             print(result)\n            if 'unusable' not in result_usability.lower():\n                print(True)\n                usable=True\n            result_type= x['type']\n            if 'z-axial-brain' in result_type.lower() or 'z-brain-thin' in result_type.lower():\n                print(True)\n                brain_type=True\n            break\n    if usable==True and brain_type==True:\n        decision =True\n    return decision\n\n\n\n\n# result = response.json()['ResultSet']['Result']\n# # print(result[0]) #['absolutePath'])\n# nDicomFiles = len(result)\n# # print(nDicomFiles)\n# if nDicomFiles == 0:\n#     raise Exception(\"No DICOM files for %s stored in XNAT\" % scanId)\n\n\ndef get_nifti_using_xnat(sessionId, scanId):\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    url = (\"/data/experiments/%s/scans/%s/resources/NIFTI/files?format=zip\" %\n        (sessionId, scanId))\n\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    zipfilename=os.path.join('/workinginput',sessionId+scanId+'.zip')\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n    subprocess.call(command,shell=True)\n    command='rm -r ' + zipfilename\n    command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n    subprocess.call(command,shell=True)\n\n    return True\ndef call_downloadfiletolocaldir_py(args):\n    sessionId=args.stuff[1]\n    scanId=args.stuff[2]\n    resource_dirname=args.stuff[3]\n    output_dirname=args.stuff[4]\n    downloadfiletolocaldir_py(sessionId,scanId,resource_dirname,output_dirname)\ndef downloadfiletolocaldir_py(sessionId,scanId,resource_dirname,output_dirname):\n    # print(sys.argv)\n    # sessionId=str(sys.argv[1])\n    # scanId=str(sys.argv[2])\n    # resource_dirname=str(sys.argv[3])\n    # output_dirname=str(sys.argv[4])\n\n    print('sessionId::scanId::resource_dirname::output_dirname::{}::{}::{}::{}'.format(sessionId,scanId,resource_dirname,output_dirname))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    resource_dir_url=((\"/data/experiments/%s/scans/%s\")  %\n                      (sessionId, scanId))\n    print('resource_dir_url::{}'.format(resource_dir_url))\n    # resource_metadata=get_resourcefiles_metadata(resource_dir_url,resource_dirname)\n    # df_scan = pd.read_json(json.dumps(resource_metadata))\n    # print('df_scan::{}'.format(resource_metadata))\n    url = ((\"/data/experiments/%s/scans/%s/resources/\" + resource_dirname+ \"/files?format=zip\")  %\n           (sessionId, scanId))\n\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    zipfilename=sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    command='rm -r /ZIPFILEDIR/* '\n    subprocess.call(command,shell=True)\n    command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n    subprocess.call(command,shell=True)\n    #xnatSession.close_httpsession())\n    copy_nifti_to_a_dir(output_dirname)\n    copy_mat_to_a_dir(output_dirname)\n    copy_allfile_to_a_dir(output_dirname)\n\n    return True\n\n\ndef downloadfiletolocaldir():\n    print(sys.argv)\n    sessionId=str(sys.argv[1])\n    scanId=str(sys.argv[2])\n    resource_dirname=str(sys.argv[3])\n    output_dirname=str(sys.argv[4])\n\n    print('sessionId::scanId::resource_dirname::output_dirname::{}::{}::{}::{}'.format(sessionId,scanId,resource_dirname,output_dirname))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    resource_dir_url=((\"/data/experiments/%s/scans/%s\")  %\n        (sessionId, scanId))\n    print('resource_dir_url::{}'.format(resource_dir_url))\n    # resource_metadata=get_resourcefiles_metadata(resource_dir_url,resource_dirname)\n    # df_scan = pd.read_json(json.dumps(resource_metadata))\n    # print('df_scan::{}'.format(resource_metadata))\n    url = ((\"/data/experiments/%s/scans/%s/resources/\" + resource_dirname+ \"/files?format=zip\")  %\n        (sessionId, scanId))\n\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    zipfilename=sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    command='rm -r /ZIPFILEDIR/* '\n    subprocess.call(command,shell=True)\n    command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n    subprocess.call(command,shell=True)\n    #xnatSession.close_httpsession())\n    copy_nifti_to_a_dir(output_dirname)\n    copy_mat_to_a_dir(output_dirname)\n\n    return True\n# def downloadfiletolocaldir_py(sessionId,scanId,resource_dirname,output_dirname):\n#     #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n#     url = ((\"/data/experiments/%s/scans/%s/resources/\" + resource_dirname+ \"/files?format=zip\")  %\n#         (sessionId, scanId))\n\n#     #xnatSession.renew_httpsession()\n#     response = xnatSession.httpsess.get(xnatSession.host + url)\n#     zipfilename=sessionId+scanId+'.zip'\n#     with open(zipfilename, \"wb\") as f:\n#         for chunk in response.iter_content(chunk_size=512):\n#             if chunk:  # filter out keep-alive new chunks\n#                 f.write(chunk)\n#     command='rm -r /ZIPFILEDIR/* '\n#     subprocess.call(command,shell=True)\n#     command = 'unzip -d /ZIPFILEDIR ' + zipfilename\n#     subprocess.call(command,shell=True)\n#     #xnatSession.close_httpsession())\n#     copy_nifti_to_a_dir(output_dirname)\ndef copy_allfile_to_a_dir(dir_name):\n    for dirpath, dirnames, files in os.walk('/ZIPFILEDIR'):\n        #                print(f'Found directory: {dirpath}')\n        for file_name in files:\n            # file_extension = pathlib.Path(file_name).suffix\n        # if  file_extension:\n            command='cp ' + os.path.join(dirpath,file_name) + '  ' + dir_name + '/'\n            subprocess.call(command,shell=True)\n            print(os.path.join(dirpath,file_name))\n\n#     return True\ndef copy_nifti_to_a_dir(dir_name):\n    for dirpath, dirnames, files in os.walk('/ZIPFILEDIR'):\n    #                print(f'Found directory: {dirpath}')\n        for file_name in files:\n            file_extension = pathlib.Path(file_name).suffix\n            if 'nii' in file_extension or 'gz' in file_extension:\n                command='cp ' + os.path.join(dirpath,file_name) + '  ' + dir_name + '/'\n                subprocess.call(command,shell=True)\n                print(os.path.join(dirpath,file_name))\ndef copy_mat_to_a_dir(dir_name):\n    for dirpath, dirnames, files in os.walk('/ZIPFILEDIR'):\n        #                print(f'Found directory: {dirpath}')\n        for file_name in files:\n            file_extension = pathlib.Path(file_name).suffix\n            if '.mat' in file_extension :\n                command='cp ' + os.path.join(dirpath,file_name) + '  ' + dir_name + '/'\n                subprocess.call(command,shell=True)\n                print(os.path.join(dirpath,file_name))\n\n## load two files:\ndef list_analyzed_session(pdffilelist_file,selectedniftifilelist_file,allsessionlist_file,output_list_csvfile,pdffilelist_file_ext=\".pdf\",stringtofilterallsessionlist=''):\n    file1=pdffilelist_file #\"workingoutput/allfilesinprojectoutput.csv\"\n    file2=selectedniftifilelist_file #\"workingoutput/COLI_EDEMA_BIOMARKER_ANALYZED.csv\"\n    file3=allsessionlist_file #\"workingoutput/all_sessions.csv\"\n    df1=pd.read_csv(file1)\n    df2=pd.read_csv(file2)\n    columname='NIFTIFILENAME'\n    df1[columname] = df1['Name']\n    df1[['NIFTIFILENAME','RESTOFTHENAME']] =df1.NIFTIFILENAME.str.split(pdffilelist_file_ext, expand = True)\n    # # aa.to_csv(csvfile,index=False)\n    df1=df1[df1['Name'].str.contains('.pdf')]\n    df2['SESSION_ID'] = df2['URI'].str.split('/').str[3]\n    # # df2['NIFTIFILENAME'] = df2['URI'].str.split('/').str[9].split('.nii')[0]\n    columname='NIFTIFILENAME'\n    df2[columname] = df2['Name']\n    df2[['NIFTIFILENAME','RESTOFTHENAME']] =df2.NIFTIFILENAME.str.split(\".nii\", expand = True)\n    df3 = pd.merge(df1, df2, left_on='NIFTIFILENAME', right_on='NIFTIFILENAME')\n    df3=df3[['NIFTIFILENAME','SESSION_ID']]\n    df4=pd.read_csv(file3)\n    df4=df4[df4['label'].str.contains(stringtofilterallsessionlist)]\n    df4['SESSION_NAME']=df4[['label']]\n    df4=df4[['ID','SESSION_NAME']]\n    # df4=df4[['ID','label']]\n\n    df5 = pd.merge(df4, df3, right_on='SESSION_ID', left_on='ID',how='outer')\n    # print(df3.shape)\n    # print(df4.shape)\n    # print(df5.shape)\n    df5['SESSION_ID']=df5[['ID']]\n    df5=df5[['SESSION_ID','NIFTIFILENAME','SESSION_NAME']]\n    # print(df5)\n    df5['ANALYZED']=0\n    df5.loc[df5[\"NIFTIFILENAME\"].str.len()>1,'ANALYZED']=1 #.value_counts()\n    df5.to_csv(output_list_csvfile,index=False)\n    # # df5.loc[df[\"NIFTIFILENAME\"].str.len() > 1 , \"gender\"] = 1\n    print(df5)\ndef call_list_analyzed_session():\n    pdffilelist_file =sys.argv[1] #\"workingoutput/allfilesinprojectoutput.csv\"\n    selectedniftifilelist_file =sys.argv[2]  #\"workingoutput/COLI_EDEMA_BIOMARKER_ANALYZED.csv\"\n    allsessionlist_file = sys.argv[3]  #\"workingoutput/all_sessions.csv\"\n    output_list_csvfile=sys.argv[4]  #\"workingoutput/all_sessions_labeled.csv\"\n    pdffilelist_file_ext=sys.argv[5]\n    stringtofilterallsessionlist=sys.argv[6]\n    list_analyzed_session(pdffilelist_file,selectedniftifilelist_file,allsessionlist_file,output_list_csvfile,pdffilelist_file_ext,stringtofilterallsessionlist)\n\n\ndef check_if_a_file_exist_in_snipr(URI, resource_dir,extension_to_find_list):\n    url = (URI+'/resources/' + resource_dir +'/files?format=json')\n    # print(\"url::{}\".format(url))\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    response = xnatSession.httpsess.get(xnatSession.host + url)\n    num_files_present=0\n    if response.status_code != 200:\n        #xnatSession.close_httpsession())\n        return num_files_present\n    metadata_masks=response.json()['ResultSet']['Result']\n    # print(\"metadata_masks::{}\".format(metadata_masks))\n    df_scan = pd.read_json(json.dumps(metadata_masks))\n    for extension_to_find in extension_to_find_list:\n        for x in range(df_scan.shape[0]):\n            print(df_scan.loc[x,'Name'])\n            if extension_to_find in df_scan.loc[x,'Name']:\n                num_files_present=num_files_present+1\n                break\n    return num_files_present\ndef print_hosts():\n    print(XNAT_HOST)\n    print(XNAT_USER)\n    print(XNAT_PASS)\n    URI=\"/data/experiments/SNIPR01_E00131/scans/3\"\n    resource_dir=\"MASKS\"\n    extension_to_find_list=[\"levelset\"]\n    num_files_present=check_if_a_file_exist_in_snipr(URI, resource_dir,extension_to_find_list)\n    if num_files_present < len(extension_to_find_list):\n        print(\"REQUIRED NUMBER OF FILES NOT PRESENT\")\n    else:\n        print(\"FILES PRESENT\")\ndef call_check_if_a_file_exist_in_snipr( args):\n    # parser = argparse.ArgumentParser()\n    # parser.add_argument('stuff', nargs='+')\n    # args = parser.parse_args()\n    # print (args.stuff)\n    returnvalue=0\n    try:\n        sessionID=args.stuff[1]\n        scanID=args.stuff[2]\n        resource_dir=args.stuff[3]\n        URI=\"/data/experiments/\"+sessionID+\"/scans/\"+scanID\n        extension_to_find_list=args.stuff[4:]\n        file_present=check_if_a_file_exist_in_snipr(URI, resource_dir,extension_to_find_list)\n        ############\n        all_files_present_flag=0\n        if file_present < len(extension_to_find_list):\n            subprocess.call(\"echo \" + \"I PASSED 0 AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n            return 0\n        else:\n            all_files_present_flag=1\n        all_files_present_flag_df=pd.DataFrame([all_files_present_flag])\n        all_files_present_flag_df.columns=['all_files_present_flag']\n        all_files_present_flag_df.to_csv('/workinginput/all_files_present_flag_df.csv',index=False)\n        returnvalue=1\n        ########\n        # if file_present < len(extension_to_find_list):\n        #     pass\n        # else:\n        #     returnvalue=1\n        # return 1\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n    except:\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        pass\n    return  returnvalue\n\ndef get_latest_file_for_analytics(df_listfile): ##,SCAN_URI_NIFTI_FILEPREFIX=\"\"):\n    allfileswithprefix1_df=df_listfile\n    # if len(SCAN_URI_NIFTI_FILEPREFIX)>0:\n    #     allfileswithprefix1_df=allfileswithprefix1_df[allfileswithprefix1_df.URI.str.contains(SCAN_URI_NIFTI_FILEPREFIX)]\n    allfileswithprefix1_df[\"FILE_BASENAME\"]=allfileswithprefix1_df[\"URI\"].apply(os.path.basename)\n\n    # allfileswithprefix1_df['FILE_BASENAME']=allfileswithprefix1_df[\"FILENAME\"].apply(os.path.basename)\n    allfileswithprefix1_df['DATE']=allfileswithprefix1_df['FILE_BASENAME']\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str[-18:-4]\n    allfileswithprefix1_df['DATETIME'] =    allfileswithprefix1_df['DATE']\n    allfileswithprefix1_df['DATETIME'] = pd.to_datetime(allfileswithprefix1_df['DATETIME'], format='%Y%m%d%H%M%S', errors='coerce')\n    allfileswithprefix1_df = allfileswithprefix1_df.sort_values(by=['DATETIME'], ascending=False)\n    # print(allfileswithprefix1_df[\"DATETIME\"])\n    allfileswithprefix1_df=allfileswithprefix1_df.reset_index(drop=True)\n    x_df=allfileswithprefix1_df.iloc[[0]]\n    return x_df\n\ndef get_latest_file(df_listfile): ##,SCAN_URI_NIFTI_FILEPREFIX=\"\"):\n    allfileswithprefix1_df=df_listfile\n    # if len(SCAN_URI_NIFTI_FILEPREFIX)>0:\n    #     allfileswithprefix1_df=allfileswithprefix1_df[allfileswithprefix1_df.URI.str.contains(SCAN_URI_NIFTI_FILEPREFIX)]\n    allfileswithprefix1_df[\"FILE_BASENAME\"]=allfileswithprefix1_df[\"URI\"].apply(os.path.basename)\n\n    # allfileswithprefix1_df['FILE_BASENAME']=allfileswithprefix1_df[\"FILENAME\"].apply(os.path.basename)\n    allfileswithprefix1_df['DATE']=allfileswithprefix1_df['FILE_BASENAME']\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str[-16:-4]\n    allfileswithprefix1_df['DATETIME'] =    allfileswithprefix1_df['DATE']\n    allfileswithprefix1_df['DATETIME'] = pd.to_datetime(allfileswithprefix1_df['DATETIME'], format='%Y%m%d%H%M', errors='coerce')\n    allfileswithprefix1_df = allfileswithprefix1_df.sort_values(by=['DATETIME'], ascending=False)\n    # print(allfileswithprefix1_df[\"DATETIME\"])\n    allfileswithprefix1_df=allfileswithprefix1_df.reset_index(drop=True)\n    x_df=allfileswithprefix1_df.iloc[[0]]\n    return x_df\n\ndef download_a_singlefile_with_URLROW(url,dir_to_save):\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    # command=\"echo  \" + url['URI'] + \" >> \" +  os.path.join(dir_to_save,\"test.csv\")\n    # subprocess.call(command,shell=True)\n    response = xnatSession.httpsess.get(xnatSession.host +url.loc[0,\"URI\"]) #/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\") #\n    #                                                       # \"/data/experiments/SNIPR02_E03548/scans/1-CT1/resources/147851/files/ICH_0001_01022017_0414_1-CT1_threshold-1024.0_22121.0TOTAL_VersionDate-11302022_04_22_2023.csv\") ## url['URI'])\n    zipfilename=os.path.join(dir_to_save,os.path.basename(url.loc[0,\"Name\"]) ) #\"/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\")) #sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    #xnatSession.close_httpsession())\n    return zipfilename\ndef get_latest_file_SAH(df_listfile): ##,SCAN_URI_NIFTI_FILEPREFIX=\"\"):\n    allfileswithprefix1_df=df_listfile\n    # if len(SCAN_URI_NIFTI_FILEPREFIX)>0:\n    #     allfileswithprefix1_df=allfileswithprefix1_df[allfileswithprefix1_df.URI.str.contains(SCAN_URI_NIFTI_FILEPREFIX)]\n    allfileswithprefix1_df[\"FILE_BASENAME\"]=allfileswithprefix1_df[\"URI\"].apply(os.path.basename)\n\n    # allfileswithprefix1_df['FILE_BASENAME']=allfileswithprefix1_df[\"FILENAME\"].apply(os.path.basename)\n    allfileswithprefix1_df['DATE']=allfileswithprefix1_df['FILE_BASENAME']\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str[-14:-4]\n    allfileswithprefix1_df['DATETIME'] =    allfileswithprefix1_df['DATE']\n    allfileswithprefix1_df['DATETIME'] = pd.to_datetime(allfileswithprefix1_df['DATETIME'], format='%m_%d_%Y', errors='coerce')\n    allfileswithprefix1_df = allfileswithprefix1_df.sort_values(by=['DATETIME'], ascending=False)\n    # print(allfileswithprefix1_df[\"DATETIME\"])\n    allfileswithprefix1_df=allfileswithprefix1_df.reset_index(drop=True)\n    x_df=allfileswithprefix1_df.iloc[[0]]\n    return x_df\ndef get_latest_file_ICH_CSV_COLDROPPED(df_listfile): ##,SCAN_URI_NIFTI_FILEPREFIX=\"\"):\n    allfileswithprefix1_df=df_listfile\n    # if len(SCAN_URI_NIFTI_FILEPREFIX)>0:\n    #     allfileswithprefix1_df=allfileswithprefix1_df[allfileswithprefix1_df.URI.str.contains(SCAN_URI_NIFTI_FILEPREFIX)]\n    allfileswithprefix1_df[\"FILE_BASENAME\"]=allfileswithprefix1_df[\"URI\"].apply(os.path.basename)\n\n    # allfileswithprefix1_df['FILE_BASENAME']=allfileswithprefix1_df[\"FILENAME\"].apply(os.path.basename)\n    allfileswithprefix1_df['DATE']=allfileswithprefix1_df['FILE_BASENAME']\n    allfileswithprefix1_df['DATE'] = allfileswithprefix1_df['DATE'].str[(-14-13):(-4-13)]\n    allfileswithprefix1_df['DATETIME'] =    allfileswithprefix1_df['DATE']\n    allfileswithprefix1_df['DATETIME'] = pd.to_datetime(allfileswithprefix1_df['DATETIME'], format='%m_%d_%Y', errors='coerce')\n    allfileswithprefix1_df = allfileswithprefix1_df.sort_values(by=['DATETIME'], ascending=False)\n    # print(allfileswithprefix1_df[\"DATETIME\"])\n    allfileswithprefix1_df=allfileswithprefix1_df.reset_index(drop=True)\n    x_df=allfileswithprefix1_df.iloc[[0]]\n    return x_df\n\ndef call_download_a_singlefile_with_URIString(args):\n    url=args.stuff[1]\n    filename=args.stuff[2]\n    dir_to_save=args.stuff[3]\n    download_a_singlefile_with_URIString(url,filename,dir_to_save)\n    return\ndef delete_a_file_with_URIString(url):\n    try:\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n        response = xnatSession.httpsess.delete(xnatSession.host +url)\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}::RESPONSE::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3],response) ,shell=True )\n    except:\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        pass\n\ndef download_a_singlefile_with_URIString(url,filename,dir_to_save):\n    print(\"url::{}::filename::{}::dir_to_save::{}\".format(url,filename,dir_to_save))\n    # #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    # command=\"echo  \" + url['URI'] + \" >> \" +  os.path.join(dir_to_save,\"test.csv\")\n    # subprocess.call(command,shell=True)\n    response = xnatSession.httpsess.get(xnatSession.host +url) #/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\") #\n    #                                                       # \"/data/experiments/SNIPR02_E03548/scans/1-CT1/resources/147851/files/ICH_0001_01022017_0414_1-CT1_threshold-1024.0_22121.0TOTAL_VersionDate-11302022_04_22_2023.csv\") ## url['URI'])\n    zipfilename=os.path.join(dir_to_save,filename ) #\"/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\")) #sessionId+scanId+'.zip'\n    with open(zipfilename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    # #xnatSession.close_httpsession())\n    return zipfilename\ndef download_an_xmlfile_with_URIString(args): #url,filename,dir_to_save):\n    returnvalue=0\n\n    try:\n        session_ID=str(args.stuff[1])\n        filename=str(args.stuff[2])\n        dir_to_save=str(args.stuff[3])\n        subprocess.call('echo working:' +session_ID+' > /workingoutput/testatul.txt',shell=True)\n        subprocess.call('echo working:' +filename+' > /workingoutput/testatul.txt',shell=True)\n        subprocess.call('echo working:' +dir_to_save+' > /workingoutput/testatul.txt',shell=True)\n        print(\"url::{}::filename::{}::dir_to_save::{}\".format(session_ID,filename,dir_to_save))\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n\n        # command=\"echo  \" + url['URI'] + \" >> \" +  os.path.join(dir_to_save,\"test.csv\")\n        # subprocess.call(command,shell=True) https://snipr.wustl.edu/app/action/XDATActionRouter/xdataction/xml/search_element/xnat%3ActSessionData/search_field/xnat%3ActSessionData.ID/search_value/SNIPR01_E07665\n        # https://snipr.wustl.edu/app/action/XDATActionRouter/xdataction/xml_file/search_element/xnat%3ActSessionData/search_field/xnat%3ActSessionData.ID/search_value/SNIPR02_E03847\n        url='/app/action/XDATActionRouter/xdataction/xml_file/search_element/xnat%3ActSessionData/search_field/xnat%3ActSessionData.ID/search_value/'+str(session_ID)  ##+'/popup/false/project/ICH'\n        subprocess.call(\"echo \" + \"I url AT ::{}  >> /workingoutput/error.txt\".format(xnatSession.host +url) ,shell=True )\n        xmlfilename=os.path.join(dir_to_save,filename )\n        try:\n            response = xnatSession.httpsess.get(xnatSession.host +url) #/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\") #\n            num_files_present=0\n            subprocess.call(\"echo \" + \"I response AT ::{}  >> /workingoutput/error.txt\".format(response) ,shell=True )\n            metadata_masks=response.text #json()['ResultSet']['Result']\n            f = open(xmlfilename, \"w\")\n            f.write(metadata_masks)\n            f.close()\n            # if response.status_code != 200:\n        except:\n            command='curl -u '+ XNAT_USER +':'+XNAT_PASS+' -X GET '+ xnatSession.host +url + ' > '+ xmlfilename\n            subprocess.call(command,shell=True)\n            subprocess.call(\"echo \" + \"I LINE 2096 AT ::{}::{}  >> /workingoutput/error.txt\".format(xmlfilename,inspect.stack()[0][3]) ,shell=True )\n            print(\"I PASSED AT ::{}\".format(inspect.stack()[0][3]))\n        # #xnatSession.close_httpsession())\n            # return num_files_present\n\n\n\n        # zipfilename=os.path.join(dir_to_save,filename ) #\"/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\")) #sessionId+scanId+'.zip'\n        # with open(zipfilename, \"wb\") as f:\n        #     for chunk in response.iter_content(chunk_size=512):\n        #         if chunk:  # filter out keep-alive new chunks\n        #             f.write(chunk)\n        #xnatSession.close_httpsession())\n        returnvalue=1\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        print(\"I PASSED AT ::{}\".format(inspect.stack()[0][3]))\n    except:\n\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n    return returnvalue\n\ndef download_an_xmlfile_with_URIString_func(session_ID,filename,dir_to_save): #url,filename,dir_to_save):\n    returnvalue=0\n\n    try:\n        # session_ID=str(args.stuff[1])\n        # filename=str(args.stuff[2])\n        # dir_to_save=str(args.stuff[3])\n        subprocess.call('echo working:' +session_ID+' > /workingoutput/testatul.txt',shell=True)\n        subprocess.call('echo working:' +filename+' > /workingoutput/testatul.txt',shell=True)\n        subprocess.call('echo working:' +dir_to_save+' > /workingoutput/testatul.txt',shell=True)\n        print(\"url::{}::filename::{}::dir_to_save::{}\".format(session_ID,filename,dir_to_save))\n        #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n        #xnatSession.renew_httpsession()\n\n        # command=\"echo  \" + url['URI'] + \" >> \" +  os.path.join(dir_to_save,\"test.csv\")\n        # subprocess.call(command,shell=True)\n        # https://snipr.wustl.edu/app/action/XDATActionRouter/xdataction/xml_file/search_element/xnat%3ActSessionData/search_field/xnat%3ActSessionData.ID/search_value/SNIPR02_E03847\n        url='/app/action/XDATActionRouter/xdataction/xml_file/search_element/xnat%3ActSessionData/search_field/xnat%3ActSessionData.ID/search_value/'+str(session_ID)  ##+'/popup/false/project/ICH'\n        subprocess.call(\"echo \" + \"I url AT ::{}  >> /workingoutput/error.txt\".format(xnatSession.host +url) ,shell=True )\n        xmlfilename=os.path.join(dir_to_save,filename )\n        try:\n            response = xnatSession.httpsess.get(xnatSession.host +url) #/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\") #\n            num_files_present=0\n            subprocess.call(\"echo \" + \"I response AT ::{}  >> /workingoutput/error.txt\".format(response) ,shell=True )\n            metadata_masks=response.text #json()['ResultSet']['Result']\n            f = open(xmlfilename, \"w\")\n            f.write(metadata_masks)\n            f.close()\n            # if response.status_code != 200:\n        except:\n            command='curl -u '+ XNAT_USER +':'+XNAT_PASS+' -X GET '+ xnatSession.host +url + ' > '+ xmlfilename\n            subprocess.call(command,shell=True)\n        # #xnatSession.close_httpsession())\n        # return num_files_present\n\n\n\n        # zipfilename=os.path.join(dir_to_save,filename ) #\"/data/projects/ICH/resources/179772/files/ICH_CTSESSIONS_202305170753.csv\")) #sessionId+scanId+'.zip'\n        # with open(zipfilename, \"wb\") as f:\n        #     for chunk in response.iter_content(chunk_size=512):\n        #         if chunk:  # filter out keep-alive new chunks\n        #             f.write(chunk)\n        #xnatSession.close_httpsession())\n        returnvalue=1\n        subprocess.call(\"echo \" + \"I PASSED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        print(\"I PASSED AT ::{}\".format(inspect.stack()[0][3]))\n    except:\n\n        subprocess.call(\"echo \" + \"I FAILED AT ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n        print(\"I PASSED AT ::{}\".format(inspect.stack()[0][3]))\n    return returnvalue\n\n\ndef listoffile_witha_URI_as_df(URI):\n    # #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n    # print(\"I AM IN :: listoffile_witha_URI_as_df::URI::{}\".format(URI))\n\n    response = xnatSession.httpsess.get(xnatSession.host + URI)\n    # print(\"I AM IN :: listoffile_witha_URI_as_df::URI::{}\".format(URI))\n    num_files_present=0\n    df_scan=[]\n    if response.status_code != 200:\n        # #xnatSession.close_httpsession())\n        return num_files_present\n    # subprocess.call(\"echo \" + \"I PASSED ATUL  ::{}  >> /workingoutput/error.txt\".format(inspect.stack()[0][3]) ,shell=True )\n    metadata_masks=response.json()['ResultSet']['Result']\n    df_listfile = pd.read_json(json.dumps(metadata_masks))\n    # #xnatSession.close_httpsession())\n    return df_listfile\ndef download_files_in_a_resource(URI,dir_to_save):\n    try:\n        df_listfile=listoffile_witha_URI_as_df(URI)\n        print(\"df_listfile::{}\".format(df_listfile))\n        # download_a_singlefile_with_URLROW(df_listfile,dir_to_save)\n        for item_id, row in df_listfile.iterrows():\n            # print(\"row::{}\".format(row))\n            # download_a_singlefile_with_URLROW(row,dir_to_save)\n            download_a_singlefile_with_URIString(row['URI'],row['Name'],dir_to_save)\n            print(\"DOWNLOADED ::{}\".format(row))\n    except:\n        print(\"FAILED AT ::{}\".format(\"download_files_in_a_resource\"))\n        pass\ndef download_files_in_scans_resources_withname_sh():\n    sessionId=sys.argv[1]\n    scan_id=sys.argv[2]\n    resource_dirname=sys.argv[3]\n    dir_to_save=sys.argv[4]\n    try:\n        URI = ((\"/data/experiments/%s\")  %\n               (sessionId))\n        session_meta_data=get_metadata_session(URI)\n        session_meta_data_df = pd.read_json(json.dumps(session_meta_data))\n        for index, row in session_meta_data_df.iterrows():\n\n            URI = ((row[\"URI\"]+\"/resources/\" + resource_dirname+ \"/files?format=json\")  %\n               (sessionId))\n            df_listfile=listoffile_witha_URI_as_df(URI)\n            print(\"df_listfile::{}\".format(df_listfile))\n        # download_a_singlefile_with_URLROW(df_listfile,dir_to_save)\n            for item_id, row in df_listfile.iterrows():\n                if str(row[\"ID\"])==str(scan_id):\n                    # print(\"row::{}\".format(row))\n                    # download_a_singlefile_with_URLROW(row,dir_to_save)\n                    download_a_singlefile_with_URIString(row['URI'],row['Name'],dir_to_save)\n                    print(\"DOWNLOADED ::{}\".format(row))\n                    print(\"PASSED AT ::{}\".format(\"download_files_in_a_resource\"))\n\n    except:\n        print(\"FAILED AT ::{}\".format(\"download_files_in_a_resource\"))\n        pass\n\n   \ndef download_files_in_a_resource_withname(sessionId,resource_dirname,dir_to_save):\n    try:\n        URI = ((\"/data/experiments/%s/resources/\" + resource_dirname+ \"/files?format=json\")  %\n               (sessionId))\n        df_listfile=listoffile_witha_URI_as_df(URI)\n        print(\"df_listfile::{}\".format(df_listfile))\n        # download_a_singlefile_with_URLROW(df_listfile,dir_to_save)\n        for item_id, row in df_listfile.iterrows():\n            # print(\"row::{}\".format(row))\n            # download_a_singlefile_with_URLROW(row,dir_to_save)\n            download_a_singlefile_with_URIString(row['URI'],row['Name'],dir_to_save)\n            print(\"DOWNLOADED ::{}\".format(row))\n    except:\n        print(\"FAILED AT ::{}\".format(\"download_files_in_a_resource\"))\n        pass\ndef call_download_files_in_a_resource_in_a_session(args):\n    returnvalue=0\n    sessionId=args.stuff[1]\n    # scanId=args.stuff[2]\n\n    resource_dirname=args.stuff[2]\n    dir_to_save=args.stuff[3]\n    URI = ((\"/data/experiments/%s/resources/\" + resource_dirname+ \"/files?format=json\")  %\n        (sessionId))\n    # Log failure message for debugging\n    command = f\"echo passed at call_download_files_in_a_resource_in_a_session: {inspect.stack()[0][3]} >> /output/error.txt\"\n    subprocess.call(command, shell=True)\n    try:\n        download_files_in_a_resource(URI,dir_to_save)\n        print(\"I AM IN :: call_download_files_in_a_resource_in_a_session::URI::{}\".format(URI))\n        return 1\n    except:\n        pass\n    return returnvalue\n    # URI,dir_to_save\ndef call_concatenate_csv_list(args):\n    all_files=args.stuff[2:]\n    outputfilename=args.stuff[1]\n    df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n    df=df.drop_duplicates()\n    df.to_csv(outputfilename,index=False)\n\ndef download_all_csv_files_givena_URIdf(URI_DF,projectname,dir_to_save):\n    try:\n        URI_DF_WITH_CSVFILES=URI_DF[URI_DF[projectname+'_CSVFILE_AVAILABLE']==1]\n        print(\"URI_DF_WITH_CSVFILESshape::{}\".format(URI_DF_WITH_CSVFILES.shape))\n        for item_id1, each_selected_scan in URI_DF_WITH_CSVFILES.iterrows():\n            download_a_singlefile_with_URIString(each_selected_scan[projectname+'_CSVFILENAME'],os.path.basename(each_selected_scan[projectname+'_CSVFILENAME']),dir_to_save)\n        print(\"I SUCCEEDED AT ::{}\".format(inspect.stack()[0][3]))\n            # pass\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n    return\ndef download_files_with_mastersessionlist(sessionlist_filename,masktype,filetype,dir_to_save,listofsession_current=\"\"):\n    try:\n        if os.path.exists(listofsession_current):\n            listofsession_current_df=pd.read_csv(listofsession_current)\n\n        sessionlist_filename_df=pd.read_csv(sessionlist_filename)\n        sessionlist_filename_df=sessionlist_filename_df[sessionlist_filename_df[masktype+'_'+filetype+'FILE_AVAILABLE']==1]\n        # print(\"URI_DF_WITH_CSVFILESshape::{}\".format(sessionlist_filename_df))\n        files_local_location=[]\n        for item_id1, each_selected_scan in sessionlist_filename_df.iterrows():\n            # print(\"CSV FILE URL::{}\".format(each_selected_scan[masktype+'_'+filetype+'FILENAME']))\n            this_filename=os.path.join(dir_to_save,os.path.basename(each_selected_scan[masktype+'_'+filetype+'FILENAME']))\n            download_a_singlefile_with_URIString(each_selected_scan[masktype+'_'+filetype+'FILENAME'],os.path.basename(each_selected_scan[masktype+'_'+filetype+'FILENAME']),dir_to_save)\n            if \".csv\" in this_filename:\n                this_filename_df=pd.read_csv(this_filename)\n                this_filename_df['SESSION_ID']=each_selected_scan[masktype+'_'+filetype+'FILENAME'].split('/')[3]\n                this_filename_df['SESSION_LABEL']=each_selected_scan['label'] ##.split('/')[3]\n                this_filename_df['SCAN_ID']=each_selected_scan[masktype+'_'+filetype+'FILENAME'].split('/')[5]\n                this_filename_df['SCAN_TYPE']=each_selected_scan['SCAN_TYPE']\n                # this_filename_df['FILEPATH'+filetype]=each_selected_scan[masktype+'_'+filetype+'FILENAME'] #.split('/')[3]\n            # listofsession_current_df_row=listofsession_current_df[listofsession_current_df['SESSION_ID']==each_selected_scan[masktype+'_'+filetype+'FILENAME'].split('/')[3]]\n            # print(\"listofsession_current exists::{}\".format(listofsession_current_df_row  ))\n                this_filename_df.to_csv(this_filename,index=False)\n            files_local_location.append(this_filename)\n        print(\"I SUCCEEDED AT ::{}\".format(inspect.stack()[0][3]))\n        return files_local_location\n\n            # pass\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n    return\ndef call_download_files_with_mastersessionlist(args):\n    try:\n        sessionlist_filename=args.stuff[1]\n        masktype=args.stuff[2]\n        filetype=args.stuff[3]\n        dir_to_save=args.stuff[4]\n        localfilelist_csv=args.stuff[5]\n        listofsession_current=args.stuff[6]\n        files_local_location=download_files_with_mastersessionlist(sessionlist_filename,masktype,filetype,dir_to_save,listofsession_current=listofsession_current)\n        files_local_location_df=pd.DataFrame(files_local_location)\n        files_local_location_df.columns=['LOCAL_FILENAME']\n        files_local_location_df.to_csv(localfilelist_csv,index=False)\n        # if upload_flag==1:\n        #     projectId=args.stuff[6]\n        #     resource_dirname=args.stuff[7]\n        #     for each_file in files_local_location:\n        #         uploadsinglefile_X_level('projects',projectId,each_file,resource_dirname)\n\n        print(\"I SUCCEEDED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n        return 0\ndef call_download_all_csv_files_givena_URIdf(args):\n    try:\n        URI_DF=pd.read_csv(args.stuff[1])\n        # scanID=args.stuff[2]\n        dir_to_save=args.stuff[2]\n        projectname=args.stuff[3]\n        print(\"URI_DF::{}::projectname::{}::dir_to_save::{}\".format(URI_DF.shape,projectname,dir_to_save))\n        download_all_csv_files_givena_URIdf(URI_DF,projectname,dir_to_save)\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\ndef divide_sessionlist_done_vs_undone(sessionlist_file,masktype):\n    try:\n        print(\"THIS FILENAME IS : {}\".format(sessionlist_file))\n        sessionlist_file_df=pd.read_csv(sessionlist_file)\n        sessionlist_file_df_done=sessionlist_file_df[sessionlist_file_df[masktype+'_PDFFILE_AVAILABLE']==1]\n        sessionlist_file_df_notdone=sessionlist_file_df[sessionlist_file_df[masktype+'_PDFFILE_AVAILABLE']!=1]\n        sessionlist_file_df_done.to_csv(sessionlist_file.split('.csv')[0]+'_done.csv' ,index=False)\n        sessionlist_file_df_notdone.to_csv(sessionlist_file.split('.csv')[0]+'_not_done.csv' ,index=False)\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n        return 0\ndef call_divide_sessionlist_done_vs_undone(args):\n    try:\n        sessionlist_file=args.stuff[1]\n        masktype=args.stuff[2]\n        divide_sessionlist_done_vs_undone(sessionlist_file,masktype)\n        print(\"I SUCCEED AT ::{}\".format(inspect.stack()[0][3]))\n        return 1\n    except:\n        print(\"I FAILED AT ::{}\".format(inspect.stack()[0][3]))\n        pass\n        return 0\ndef project_resource_latest_analytic_file(args):\n    try:\n        print(\"WO ZAI call_project_resource_latest_analytic_file try\")\n        projectID=args.stuff[1]\n        # scanID=args.stuff[2]\n        resource_dir=args.stuff[2]\n        URI=\"/data/projects/\"+projectID #+\"/scans/\"+scanID\n        URI = (URI+'/resources/' + resource_dir +'/files?format=json')\n        extension_to_find_list=args.stuff[3]\n        dir_to_save=args.stuff[4]\n        print(\"projectID::{}::resource_dir::{}::URI::{}::extension_to_find_list::{}::dir_to_save::{}\".format(projectID,resource_dir,URI,extension_to_find_list,dir_to_save))\n        df_listfile=listoffile_witha_URI_as_df(URI)\n        df_listfile=df_listfile[df_listfile.URI.str.contains(extension_to_find_list)]\n        latest_filename=get_latest_file(df_listfile)\n        print(latest_filename['URI'])\n        print(\"\\n\")\n        print(dir_to_save)\n        print(\"\\n\")\n        print(\"WO ZAI ::{}\".format(\"call_project_resource_latest_analytic_file\"))\n\n        filename_saved=download_a_singlefile_with_URLROW(latest_filename,dir_to_save)\n        # # if len(filename_saved) >0 :\n        # filename_saved_df=pd.read_csv(filename_saved)\n        # required_col = filename_saved_df.columns[filename_saved_df.columns.str.contains(pat = 'PDFFILE_AVAILABLE')]\n        # print(\"required_col:{}\".format(required_col[0]))\n        # filename_saved_df_notdone=filename_saved_df[filename_saved_df[required_col[0]]!=1]\n        # filename_saved_df_done=filename_saved_df[filename_saved_df[required_col[0]]==1]\n        # filename_notdone=os.path.join(dir_to_save,\"sessions.csv\")\n        # filename_done=os.path.join(dir_to_save,\"sessions_done.csv\")\n        # filename_saved_df_notdone.to_csv(filename_notdone,index=False)\n        # filename_saved_df_done.to_csv(filename_done,index=False)\n        return \"CSVMASTERFILE::\"+filename_saved\n    except:\n        return 0\n    #\n    # file_present=check_if_a_file_exist_in_snipr(URI, resource_dir,extension_to_find_list)\n    # if file_present < len(extension_to_find_list):\n    #     return\n# def get_file_for_second_round():\ndef get_selected_scan_info(SESSION_ID, dir_to_save):\n    # Log failure message for debugging\n    command = f\"echo passed at each_row_id: {inspect.stack()[0][3]} >> /output/error.txt\"\n    subprocess.call(command, shell=True)\n\n    # Define the URI and URL for the request\n    URI = f'/data/experiments/{SESSION_ID}'\n    url = f\"{URI}/resources/NIFTI_LOCATION/files?format=json\"\n\n    # Initialize XNAT session\n    #xnatSession = XnatSession(username=XNAT_USER, password=XNAT_PASS, host=XNAT_HOST)\n    #xnatSession.renew_httpsession()\n\n    try:\n        # Make a GET request to the XNAT server\n        response = xnatSession.httpsess.get(xnatSession.host + url)\n        response.raise_for_status()  # Raise an error for unsuccessful responses\n        metadata_masks = response.json()['ResultSet']['Result']\n\n        # Convert metadata to a DataFrame\n        df_scan = pd.read_json(json.dumps(metadata_masks))\n\n        # Log additional debug information\n        command = f\"echo passed at each_row_id: {df_scan['URI'].iloc[0]} >> /output/error.txt\"\n        subprocess.call(command, shell=True)\n\n        # Download the file specified by the URI\n        download_a_singlefile_with_URIString(\n            str(df_scan['URI'].iloc[0]),\n            os.path.basename(str(df_scan['URI'].iloc[0])),\n            dir_to_save\n        )\n\n        # Read the downloaded file to extract SCAN_ID and SCAN_NAME\n        nifti_location = pd.read_csv(\n            os.path.join(dir_to_save, os.path.basename(str(df_scan['URI'].iloc[0])))\n        )\n        SCAN_ID = str(nifti_location.loc[nifti_location.index[0], 'ID'])\n        SCAN_NAME = str(nifti_location.loc[nifti_location.index[0], 'Name'])\n\n        # Log success message\n        command = f\"echo passed at each_row_id: {inspect.stack()[0][3]} >> /output/error.txt\"\n        subprocess.call(command, shell=True  )\n\n        return SCAN_ID, SCAN_NAME\n\n    except Exception as e:\n        # Log exception details for debugging\n        command = f\"echo exception at {inspect.stack()[0][3]}: {str(e)} >> /output/error.txt\"\n        subprocess.call(command, shell=True)\n        raise e\n\n    finally:\n        print('Hello world')\n        # Close the XNAT session\n        #xnatSession.close_httpsession())\n\ndef call_download_a_file_with_ext(args):\n    command = f\"echo exception at {inspect.stack()[0][3]}: WO ZAI ZELI >> /output/error.txt\"\n    subprocess.call(command, shell=True)\n    session_id=args.stuff[1]\n    scan_id=args.stuff[2]\n    resource_dir=args.stuff[3]\n    extensions_to_download=args.stuff[4]\n    outputfolder=args.stuff[5]\n    prefix_if_any = ''\n    if len(args.stuff)>6: ##.stuff[6]:\n        prefix_if_any=args.stuff[6]\n    try:\n        download_a_file_with_ext(session_id, scan_id, resource_dir, extensions_to_download, outputfolder, prefix_if_any=prefix_if_any)\n    except Exception as e:\n        command = f\"echo exception at {inspect.stack()[0][3]}: {str(e)} >> /output/error.txt\"\n        subprocess.call(command, shell=True)\n    return 1\n\ndef download_a_file_with_ext(session_id,scan_id,resource_dir,extensions_to_download,outputfolder,prefix_if_any=''):\n    #         resource_dir='MASKS' #'NIFTI_LOCATION'\n    try:\n\n        URL='/data/experiments/'+session_id+'/scans/'+scan_id\n        metadata_masks=get_resourcefiles_metadata(URL,resource_dir)\n        df_scan = pd.read_json(json.dumps(metadata_masks))\n        #         extensions_to_delete=['_resaved_levelset_sulci_above_ventricle.nii.gz','_resaved_levelset_sulci_at_ventricle.nii.gz','_resaved_levelset_sulci_below_ventricle.nii.gz',\n        #                                  '_resaved_levelset_sulci_total.nii.gz','_resaved_levelset_ventricle_total.nii.gz']\n        # for each_extension in extensions_to_delete:\n        matched_rows = df_scan[df_scan['URI'].str.contains(extensions_to_download, case=False, na=False)]\n        command = \"echo  success at  download: \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        if len(prefix_if_any)>0:\n            matched_rows = matched_rows[matched_rows['URI'].str.contains(prefix_if_any, case=False, na=False)]\n\n        if matched_rows.shape[0]>0:\n            matched_rows=matched_rows.reset_index()\n            print(matched_rows)\n            for each_row_id,each_row in matched_rows.iterrows():\n                command = \"echo  success at  download each_row_id: \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n                subprocess.call(command,shell=True)\n                url=each_row['URI'] #matched_rows.at[0,'URI']\n                print(url)\n                download_a_singlefile_with_URIString(url,os.path.basename(url),outputfolder)\n                # print(\"DELETED::{}\".format(url))\n\n    except:\n        pass\n\n\ndef delete_file_with_ext(session_id,scan_id,resource_dir,extensions_to_delete,prefix_if_any=''):\n    #         resource_dir='MASKS' #'NIFTI_LOCATION'\n    try:\n\n        URL='/data/experiments/'+session_id+'/scans/'+scan_id\n        metadata_masks=get_resourcefiles_metadata(URL,resource_dir)\n        df_scan = pd.read_json(json.dumps(metadata_masks))\n        #         extensions_to_delete=['_resaved_levelset_sulci_above_ventricle.nii.gz','_resaved_levelset_sulci_at_ventricle.nii.gz','_resaved_levelset_sulci_below_ventricle.nii.gz',\n        #                                  '_resaved_levelset_sulci_total.nii.gz','_resaved_levelset_ventricle_total.nii.gz']\n        # for each_extension in extensions_to_delete:\n        matched_rows = df_scan[df_scan['URI'].str.contains(extensions_to_delete, case=False, na=False)]\n        command = \"echo  success at  DELETED: \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        if len(prefix_if_any)>0:\n            matched_rows = matched_rows[matched_rows['URI'].str.contains(prefix_if_any, case=False, na=False)]\n\n        if matched_rows.shape[0]>0:\n            matched_rows=matched_rows.reset_index()\n            print(matched_rows)\n            for each_row_id,each_row in matched_rows.iterrows():\n                command = \"echo  success at  DELETED each_row_id: \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n                subprocess.call(command,shell=True)\n                url=each_row['URI'] #matched_rows.at[0,'URI']\n                print(url)\n                delete_a_file_with_URIString(url)\n                print(\"DELETED::{}\".format(url))\n\n    except:\n        pass\n\ndef call_delete_file_with_ext(args): #session_id,scan_id,resource_dir,extensions_to_delete):\n    try:\n        session_id=args.stuff[1]\n        scan_id=args.stuff[2]\n        resource_dir=args.stuff[3]\n        extensions_to_delete=args.stuff[4]\n        prefix_if_any=''\n        if len(args.stuff)>5:\n\n            prefix_if_any=args.stuff[5]\n        delete_file_with_ext(session_id,scan_id,resource_dir,extensions_to_delete,prefix_if_any=prefix_if_any)\n        command = \"echo  success at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        return 1\n    except:\n        command = \"echo  failed at : \" +  inspect.stack()[0][3]  + \" >> \" + \"/output/error.txt\"\n        subprocess.call(command,shell=True)\n        pass\n        return 0\n\ndef find_selected_scan_id(session_id):\n    \"\"\"\n    Return (scan_id, scan_name) for the scan previously selected for analysis.\n\n    This reads the CSV saved under the XNAT session's NIFTI_LOCATION resource\n    (created by your decision_which_nifti/select_scan_for_analysis step).\n    Raises ValueError if not found.\n    \"\"\"\n    import json, io\n    import pandas as pd\n\n    # 1) List files under NIFTI_LOCATION for this session\n    base_uri = f\"/data/experiments/{session_id}\"\n    list_url = f\"{base_uri}/resources/NIFTI_LOCATION/files?format=json\"\n\n    resp = xnatSession.httpsess.get(xnatSession.host + list_url)\n    resp.raise_for_status()\n    results = resp.json().get(\"ResultSet\", {}).get(\"Result\", [])\n\n    if not results:\n        raise ValueError(f\"No NIFTI_LOCATION files found for session {session_id}.\")\n\n    # If multiple CSVs exist, prefer the most recently created one (if field present)\n    df_list = pd.read_json(json.dumps(results))\n    if \"created\" in df_list.columns:\n        df_list = df_list.sort_values(\"created\", ascending=False)\n\n    file_uri = df_list[\"URI\"].iloc[0]\n\n    # 2) Download the selected NIFTI_LOCATION CSV (without writing to disk)\n    file_resp = xnatSession.httpsess.get(xnatSession.host + file_uri)\n    file_resp.raise_for_status()\n\n    nifti_loc_df = pd.read_csv(io.StringIO(file_resp.text))\n\n    # Expect columns: 'ID' (scan id), 'Name' (nifti filename), etc.\n    if nifti_loc_df.empty or \"ID\" not in nifti_loc_df.columns or \"Name\" not in nifti_loc_df.columns:\n        raise ValueError(f\"NIFTI_LOCATION file for {session_id} is missing expected columns.\")\n\n    scan_id  = str(nifti_loc_df.iloc[0][\"ID\"])\n    scan_name = str(nifti_loc_df.iloc[0][\"Name\"])\n    result = f'\"SCAN_ID\"::{scan_id}::\"SCAN_NAME\"::{scan_name}'\n    return result ##\"SCAN_ID\"::{scan_id)::\"SCAN_NAME\"::{scan_name}\n\ndef get_largest_newest_csv_for_scan(session_id, scan_id, resource_label=\"ICH_PHE_QUANTIFICATION\"):\n    \"\"\"\n    From the given scan's resource, pick the CSV that is:\n      1) Largest by size (bytes), and\n      2) If there’s a tie, the newest by timestamp (created/modified).\n    Returns a dict: {\"name\": ..., \"uri\": ..., \"size\": int, \"created\": pandas.Timestamp}\n    Raises ValueError if none found.\n    \"\"\"\n    import json\n    import pandas as pd\n\n    # List files in the scan resource\n    base = f\"/data/experiments/{session_id}/scans/{scan_id}\"\n    list_url = f\"{base}/resources/{resource_label}/files?format=json\"\n\n    r = xnatSession.httpsess.get(xnatSession.host + list_url)\n    r.raise_for_status()\n    rows = r.json().get(\"ResultSet\", {}).get(\"Result\", [])\n    if not rows:\n        raise ValueError(f\"No files in resource '{resource_label}' for scan {scan_id} (session {session_id}).\")\n\n    df = pd.DataFrame(rows)\n\n    # Normalize helpful columns (size + timestamp may vary by XNAT version)\n    # Try common variations for size\n    size_cols = [c for c in df.columns if c.lower() in {\"size\",\"filesize\",\"file_size\",\"file_size_bytes\"}]\n    if size_cols:\n        size_col = size_cols[0]\n    else:\n        # Fallback: try to infer from 'URI' (not ideal). If missing, set 0.\n        size_col = \"_size_fallback\"\n        df[size_col] = 0\n\n    # Try common variations for created/modified timestamp\n    time_cols = [c for c in df.columns if c.lower() in {\"created\",\"modified\",\"last_modified\",\"timestamp\"}]\n    time_col = time_cols[0] if time_cols else None\n    if time_col:\n        df[\"_created_ts\"] = pd.to_datetime(df[time_col], errors=\"coerce\")\n    else:\n        df[\"_created_ts\"] = pd.NaT\n\n    # Prefer extension by Name; fall back to URI\n    name_col = \"Name\" if \"Name\" in df.columns else (\"name\" if \"name\" in df.columns else None)\n    uri_col  = \"URI\"  if \"URI\"  in df.columns else (\"uri\"  if \"uri\"  in df.columns else None)\n    if not uri_col:\n        raise ValueError(\"File listing is missing 'URI' field; cannot proceed.\")\n\n    def is_csv(row):\n        name = (row[name_col] if name_col else None) or \"\"\n        uri  = row[uri_col] or \"\"\n        return str(name).lower().endswith(\".csv\") or str(uri).lower().endswith(\".csv\")\n\n    csv_df = df[df.apply(is_csv, axis=1)].copy()\n    if csv_df.empty:\n        raise ValueError(f\"No CSV files found in resource '{resource_label}' for scan {scan_id}.\")\n\n    # Ensure numeric size, missing -> 0\n    csv_df[\"_size_bytes\"] = pd.to_numeric(csv_df[size_col], errors=\"coerce\").fillna(0).astype(int)\n\n    # Sort: largest size first, then newest timestamp\n    csv_df = csv_df.sort_values(by=[\"_size_bytes\",\"_created_ts\"], ascending=[False, False])\n\n    top = csv_df.iloc[0]\n    return {\n        \"name\": top[name_col] if name_col else top[uri_col].split(\"/\")[-1],\n        \"uri\":  top[uri_col],\n        \"size\": int(top[\"_size_bytes\"]),\n        \"created\": top[\"_created_ts\"],\n    }\n\n\ndef download_xnat_file_to_path(file_uri, out_path):\n    resp = xnatSession.httpsess.get(xnatSession.host + file_uri, stream=True)\n    resp.raise_for_status()\n    with open(os.path.join(out_path,os.path.basename(file_uri)), \"wb\") as f:\n        for chunk in resp.iter_content(chunk_size=1 << 16):\n            if chunk:\n                f.write(chunk)\n    return out_path\n\n\n\ndef main():\n    print(\"WO ZAI ::{}\".format(\"main\"))\n    parser = argparse.ArgumentParser()\n    parser.add_argument('stuff', nargs='+')\n    args = parser.parse_args()\n    name_of_the_function=args.stuff[0]\n    return_value=0\n    if name_of_the_function == \"call_check_if_a_file_exist_in_snipr\":\n        return_value=call_check_if_a_file_exist_in_snipr(args)\n    if name_of_the_function == \"project_resource_latest_analytic_file\":\n        print(\"WO ZAI ::{}\".format(name_of_the_function))\n        return_value=project_resource_latest_analytic_file(args)\n    if name_of_the_function == \"call_concatenate_csv_list\":\n        return_value=call_concatenate_csv_list(args)\n    if name_of_the_function == \"call_download_files_in_a_resource_in_a_session\":\n        return_value=call_download_files_in_a_resource_in_a_session(args)\n    if name_of_the_function == \"call_download_all_csv_files_givena_URIdf\":\n        return_value=call_download_all_csv_files_givena_URIdf(args)\n    if name_of_the_function == \"call_divide_sessionlist_done_vs_undone\":\n        return_value=call_divide_sessionlist_done_vs_undone(args)\n\n    if name_of_the_function == \"call_download_files_with_mastersessionlist\":\n        return_value=call_download_files_with_mastersessionlist(args)\n    if name_of_the_function==\"call_combinecsvs_inafileoflist\":\n        return_value=call_combinecsvs_inafileoflist(args)\n\n    if name_of_the_function==\"call_uploadfilesfromlistinacsv\":\n        return_value=call_uploadfilesfromlistinacsv(args)\n    if name_of_the_function==\"call_get_resourcefiles_metadata_saveascsv_args\":\n        return_value=call_get_resourcefiles_metadata_saveascsv_args(args)\n    if name_of_the_function==\"call_uploadsinglefile_with_URI\":\n        return_value=call_uploadsinglefile_with_URI(args)\n    if name_of_the_function==\"call_download_a_singlefile_with_URIString\":\n        return_value=call_download_a_singlefile_with_URIString(args)\n    if name_of_the_function==\"call_change_type_of_scan\":\n        return_value=call_change_type_of_scan(args)\n    if name_of_the_function==\"call_get_metadata_session\":\n        return_value=call_get_metadata_session(args)\n    if name_of_the_function==\"call_get_session_label\":\n        return_value=call_get_session_label(args) #\n    if name_of_the_function==\"call_downloadfiletolocaldir_py\":\n        return_value=call_downloadfiletolocaldir_py(args) #\n    if name_of_the_function==\"call_delete_file_with_ext\":\n        return_value=call_delete_file_with_ext(args)\n    if name_of_the_function==\"call_dowload_a_folder_as_zip\":\n        return_value=call_dowload_a_folder_as_zip(args)\n    if name_of_the_function==\"call_fill_google_mysql_db_with_single_value\":\n        return_value=call_fill_google_mysql_db_with_single_value(args)\n    if name_of_the_function==\"call_fill_google_mysql_db_from_csv\":\n        return_value=call_fill_google_mysql_db_from_csv(args) #\n    if name_of_the_function==\"call_get_session_project\":\n        return_value=call_get_session_project(args)\n    if name_of_the_function==\"call_pipeline_step_completed\":\n        return_value=call_pipeline_step_completed(args)\n    if name_of_the_function==\"call_download_a_file_with_ext\":\n        return_value=call_download_a_file_with_ext(args)\n\n    print(return_value) #\n    if \"call\" not in name_of_the_function:\n        globals()[args.stuff[0]](args)\n    xnatSession.close_httpsession()\n    return return_value\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/download_with_session_ID.py b/download_with_session_ID.py
--- a/download_with_session_ID.py	(revision 51d7187cf4437c7f93b9ba35f801fcadb7f1d19c)
+++ b/download_with_session_ID.py	(date 1761254473320)
@@ -2990,6 +2990,40 @@
                 f.write(chunk)
     return out_path
 
+def get_session_id_from_label(session_label):
+    """
+    Given a session label (human-readable name), return its XNAT session ID.
+
+    Example:
+        Input : "IBIO_0066_10182020_1848_3"
+        Output: "SNIPR02_E10245"
+
+    It queries the XNAT REST API: /data/experiments?format=json&label=<session_label>
+    """
+    import json
+
+    base_url = f"/data/experiments?format=json&label={session_label}"
+    url = xnatSession.host + base_url
+
+    # Ensure XNAT session is valid
+    xnatSession.renew_httpsession()
+
+    resp = xnatSession.httpsess.get(url)
+    resp.raise_for_status()
+
+    resultset = resp.json().get("ResultSet", {}).get("Result", [])
+    if not resultset:
+        raise ValueError(f"No session found for label: {session_label}")
+
+    # usually, each session_label is unique, so take the first
+    session_id = resultset[0].get("ID")
+    if not session_id:
+        raise ValueError(f"Could not find ID field for label: {session_label}")
+
+    return session_id
+
+
+
 
 
 def main():
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"b7703539-bebb-46f9-9292-022aedf7fa67\" name=\"Changes\" comment=\"New Commitdgfdgf\" />\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n    <option name=\"RESET_MODE\" value=\"HARD\" />\n  </component>\n  <component name=\"GitHubPullRequestSearchHistory\">{\n  &quot;lastFilter&quot;: {\n    &quot;state&quot;: &quot;OPEN&quot;,\n    &quot;assignee&quot;: &quot;dharlabwustl&quot;\n  }\n}</component>\n  <component name=\"GithubPullRequestsUISettings\">{\n  &quot;selectedUrlAndAccountId&quot;: {\n    &quot;url&quot;: &quot;https://github.com/dharlabwustl/EDEMA_MARKERS_PROD.git&quot;,\n    &quot;accountId&quot;: &quot;2f8a9d24-b611-434d-b5b3-3f7492dba692&quot;\n  }\n}</component>\n  <component name=\"HighlightingSettingsPerFile\">\n    <setting file=\"file://$PROJECT_DIR$/fillmaster_session_list.py\" root0=\"FORCE_HIGHLIGHTING\" />\n  </component>\n  <component name=\"IlluminatedCloudWorkspaceSettings\">\n    <option name=\"projectWorkspaceSettings\">\n      <ProjectWorkspaceSettings>\n        <option name=\"anonymousApexSettings\">\n          <AnonymousApexSettings>\n            <option name=\"savedDocuments\">\n              <list>\n                <AnonymousApexSavedDocument>\n                  <option name=\"name\" value=\"Anonymous Apex 1\" />\n                </AnonymousApexSavedDocument>\n              </list>\n            </option>\n          </AnonymousApexSettings>\n        </option>\n        <option name=\"soqlQuerySettings\">\n          <SoqlQuerySettings>\n            <option name=\"savedDocuments\">\n              <list>\n                <SoqlQuerySavedDocument>\n                  <option name=\"body\" value=\"\" />\n                  <option name=\"name\" value=\"SOQL Query 1\" />\n                </SoqlQuerySavedDocument>\n              </list>\n            </option>\n          </SoqlQuerySettings>\n        </option>\n      </ProjectWorkspaceSettings>\n    </option>\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectCodeStyleSettingsMigration\">\n    <option name=\"version\" value=\"2\" />\n  </component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;associatedIndex&quot;: 3\n}</component>\n  <component name=\"ProjectId\" id=\"2IBTAyvSRFAPUXT9t7jiTgp9aCO\" />\n  <component name=\"ProjectLevelVcsManager\">\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\n  </component>\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">{\n  &quot;keyToString&quot;: {\n    &quot;ASKED_ADD_EXTERNAL_FILES&quot;: &quot;true&quot;,\n    &quot;Python.dividemasks_into_left_right.executor&quot;: &quot;Run&quot;,\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.git.unshallow&quot;: &quot;true&quot;,\n    &quot;SHELLCHECK.PATH&quot;: &quot;/home/atul/.local/share/JetBrains/PyCharm2025.1/Shell Script/shellcheck&quot;,\n    &quot;git-widget-placeholder&quot;: &quot;master&quot;,\n    &quot;kotlin-language-version-configured&quot;: &quot;true&quot;,\n    &quot;last_opened_file_path&quot;: &quot;/media/atul/WDJan2022/WASHU_WORKS/PROJECTS/DOCKERIZE/deepregregis&quot;,\n    &quot;project.structure.last.edited&quot;: &quot;Project&quot;,\n    &quot;project.structure.proportion&quot;: &quot;0.0&quot;,\n    &quot;project.structure.side.proportion&quot;: &quot;0.2&quot;,\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;\n  }\n}</component>\n  <component name=\"RunManager\">\n    <configuration name=\"dividemasks_into_left_right\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EDEMA_MARKERS\" />\n      <option name=\"ENV_FILES\" value=\"\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/dividemasks_into_left_right.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n  </component>\n  <component name=\"SharedIndexes\">\n    <attachedChunks>\n      <set>\n        <option value=\"bundled-python-sdk-e0ed3721d81e-36ea0e71a18c-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-251.25410.159\" />\n      </set>\n    </attachedChunks>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"b7703539-bebb-46f9-9292-022aedf7fa67\" name=\"Changes\" comment=\"\" />\n      <created>1669653838154</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1669653838154</updated>\n    </task>\n    <task id=\"LOCAL-03957\" summary=\"&#10;New Commit\">\n      <created>1707498514749</created>\n      <option name=\"number\" value=\"03957\" />\n      <option name=\"presentableId\" value=\"LOCAL-03957\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707498514749</updated>\n    </task>\n    <task id=\"LOCAL-03958\" summary=\"&#10;New Commit\">\n      <created>1707498980389</created>\n      <option name=\"number\" value=\"03958\" />\n      <option name=\"presentableId\" value=\"LOCAL-03958\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707498980389</updated>\n    </task>\n    <task id=\"LOCAL-03959\" summary=\"&#10;New Commit\">\n      <created>1707499240908</created>\n      <option name=\"number\" value=\"03959\" />\n      <option name=\"presentableId\" value=\"LOCAL-03959\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707499240908</updated>\n    </task>\n    <task id=\"LOCAL-03960\" summary=\"&#10;New Commit\">\n      <created>1707499507470</created>\n      <option name=\"number\" value=\"03960\" />\n      <option name=\"presentableId\" value=\"LOCAL-03960\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707499507470</updated>\n    </task>\n    <task id=\"LOCAL-03961\" summary=\"&#10;New Commit\">\n      <created>1707499600440</created>\n      <option name=\"number\" value=\"03961\" />\n      <option name=\"presentableId\" value=\"LOCAL-03961\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707499600440</updated>\n    </task>\n    <task id=\"LOCAL-03962\" summary=\"&#10;New Commit\">\n      <created>1707499689237</created>\n      <option name=\"number\" value=\"03962\" />\n      <option name=\"presentableId\" value=\"LOCAL-03962\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707499689237</updated>\n    </task>\n    <task id=\"LOCAL-03963\" summary=\"&#10;New Commit\">\n      <created>1707500318160</created>\n      <option name=\"number\" value=\"03963\" />\n      <option name=\"presentableId\" value=\"LOCAL-03963\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707500318160</updated>\n    </task>\n    <task id=\"LOCAL-03964\" summary=\"&#10;New Commit\">\n      <created>1707500471852</created>\n      <option name=\"number\" value=\"03964\" />\n      <option name=\"presentableId\" value=\"LOCAL-03964\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707500471852</updated>\n    </task>\n    <task id=\"LOCAL-03965\" summary=\"&#10;New Commit\">\n      <created>1707500878302</created>\n      <option name=\"number\" value=\"03965\" />\n      <option name=\"presentableId\" value=\"LOCAL-03965\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707500878302</updated>\n    </task>\n    <task id=\"LOCAL-03966\" summary=\"&#10;New Commit\">\n      <created>1707501005447</created>\n      <option name=\"number\" value=\"03966\" />\n      <option name=\"presentableId\" value=\"LOCAL-03966\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707501005447</updated>\n    </task>\n    <task id=\"LOCAL-03967\" summary=\"&#10;New Commit\">\n      <created>1707501055502</created>\n      <option name=\"number\" value=\"03967\" />\n      <option name=\"presentableId\" value=\"LOCAL-03967\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707501055502</updated>\n    </task>\n    <task id=\"LOCAL-03968\" summary=\"&#10;New Commit\">\n      <created>1707501253158</created>\n      <option name=\"number\" value=\"03968\" />\n      <option name=\"presentableId\" value=\"LOCAL-03968\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707501253158</updated>\n    </task>\n    <task id=\"LOCAL-03969\" summary=\"&#10;New Commit\">\n      <created>1707501718778</created>\n      <option name=\"number\" value=\"03969\" />\n      <option name=\"presentableId\" value=\"LOCAL-03969\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707501718778</updated>\n    </task>\n    <task id=\"LOCAL-03970\" summary=\"&#10;New Commit\">\n      <created>1707819801423</created>\n      <option name=\"number\" value=\"03970\" />\n      <option name=\"presentableId\" value=\"LOCAL-03970\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707819801423</updated>\n    </task>\n    <task id=\"LOCAL-03971\" summary=\"&#10;New Commit\">\n      <created>1707820108539</created>\n      <option name=\"number\" value=\"03971\" />\n      <option name=\"presentableId\" value=\"LOCAL-03971\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707820108539</updated>\n    </task>\n    <task id=\"LOCAL-03972\" summary=\"&#10;New Commit\">\n      <created>1707820120794</created>\n      <option name=\"number\" value=\"03972\" />\n      <option name=\"presentableId\" value=\"LOCAL-03972\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707820120794</updated>\n    </task>\n    <task id=\"LOCAL-03973\" summary=\"&#10;New Commit\">\n      <created>1707820238181</created>\n      <option name=\"number\" value=\"03973\" />\n      <option name=\"presentableId\" value=\"LOCAL-03973\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707820238181</updated>\n    </task>\n    <task id=\"LOCAL-03974\" summary=\"&#10;New Commit\">\n      <created>1707820347611</created>\n      <option name=\"number\" value=\"03974\" />\n      <option name=\"presentableId\" value=\"LOCAL-03974\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707820347611</updated>\n    </task>\n    <task id=\"LOCAL-03975\" summary=\"&#10;New Commit\">\n      <created>1707820491351</created>\n      <option name=\"number\" value=\"03975\" />\n      <option name=\"presentableId\" value=\"LOCAL-03975\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707820491351</updated>\n    </task>\n    <task id=\"LOCAL-03976\" summary=\"&#10;New Commit\">\n      <created>1707821429187</created>\n      <option name=\"number\" value=\"03976\" />\n      <option name=\"presentableId\" value=\"LOCAL-03976\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821429187</updated>\n    </task>\n    <task id=\"LOCAL-03977\" summary=\"&#10;New Commit\">\n      <created>1707821510971</created>\n      <option name=\"number\" value=\"03977\" />\n      <option name=\"presentableId\" value=\"LOCAL-03977\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821510971</updated>\n    </task>\n    <task id=\"LOCAL-03978\" summary=\"&#10;New Commit\">\n      <created>1707821702904</created>\n      <option name=\"number\" value=\"03978\" />\n      <option name=\"presentableId\" value=\"LOCAL-03978\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821702904</updated>\n    </task>\n    <task id=\"LOCAL-03979\" summary=\"&#10;New Commit\">\n      <created>1707821757861</created>\n      <option name=\"number\" value=\"03979\" />\n      <option name=\"presentableId\" value=\"LOCAL-03979\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821757861</updated>\n    </task>\n    <task id=\"LOCAL-03980\" summary=\"&#10;New Commit\">\n      <created>1707821884226</created>\n      <option name=\"number\" value=\"03980\" />\n      <option name=\"presentableId\" value=\"LOCAL-03980\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821884226</updated>\n    </task>\n    <task id=\"LOCAL-03981\" summary=\"&#10;New Commit\">\n      <created>1707821923856</created>\n      <option name=\"number\" value=\"03981\" />\n      <option name=\"presentableId\" value=\"LOCAL-03981\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707821923856</updated>\n    </task>\n    <task id=\"LOCAL-03982\" summary=\"&#10;New Commit\">\n      <created>1707822073006</created>\n      <option name=\"number\" value=\"03982\" />\n      <option name=\"presentableId\" value=\"LOCAL-03982\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707822073006</updated>\n    </task>\n    <task id=\"LOCAL-03983\" summary=\"&#10;New Commit\">\n      <created>1707822146499</created>\n      <option name=\"number\" value=\"03983\" />\n      <option name=\"presentableId\" value=\"LOCAL-03983\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707822146499</updated>\n    </task>\n    <task id=\"LOCAL-03984\" summary=\"&#10;New Commit\">\n      <created>1707822504904</created>\n      <option name=\"number\" value=\"03984\" />\n      <option name=\"presentableId\" value=\"LOCAL-03984\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707822504904</updated>\n    </task>\n    <task id=\"LOCAL-03985\" summary=\"&#10;New Commit\">\n      <created>1707822839359</created>\n      <option name=\"number\" value=\"03985\" />\n      <option name=\"presentableId\" value=\"LOCAL-03985\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707822839359</updated>\n    </task>\n    <task id=\"LOCAL-03986\" summary=\"&#10;New Commit\">\n      <created>1707823009124</created>\n      <option name=\"number\" value=\"03986\" />\n      <option name=\"presentableId\" value=\"LOCAL-03986\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823009124</updated>\n    </task>\n    <task id=\"LOCAL-03987\" summary=\"&#10;New Commit\">\n      <created>1707823132263</created>\n      <option name=\"number\" value=\"03987\" />\n      <option name=\"presentableId\" value=\"LOCAL-03987\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823132263</updated>\n    </task>\n    <task id=\"LOCAL-03988\" summary=\"&#10;New Commit\">\n      <created>1707823392695</created>\n      <option name=\"number\" value=\"03988\" />\n      <option name=\"presentableId\" value=\"LOCAL-03988\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823392695</updated>\n    </task>\n    <task id=\"LOCAL-03989\" summary=\"&#10;New Commit\">\n      <created>1707823483842</created>\n      <option name=\"number\" value=\"03989\" />\n      <option name=\"presentableId\" value=\"LOCAL-03989\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823483842</updated>\n    </task>\n    <task id=\"LOCAL-03990\" summary=\"&#10;New Commit\">\n      <created>1707823638505</created>\n      <option name=\"number\" value=\"03990\" />\n      <option name=\"presentableId\" value=\"LOCAL-03990\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823638505</updated>\n    </task>\n    <task id=\"LOCAL-03991\" summary=\"&#10;New Commit\">\n      <created>1707823922079</created>\n      <option name=\"number\" value=\"03991\" />\n      <option name=\"presentableId\" value=\"LOCAL-03991\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707823922079</updated>\n    </task>\n    <task id=\"LOCAL-03992\" summary=\"&#10;New Commit\">\n      <created>1707824376042</created>\n      <option name=\"number\" value=\"03992\" />\n      <option name=\"presentableId\" value=\"LOCAL-03992\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707824376042</updated>\n    </task>\n    <task id=\"LOCAL-03993\" summary=\"&#10;New Commit\">\n      <created>1707824906333</created>\n      <option name=\"number\" value=\"03993\" />\n      <option name=\"presentableId\" value=\"LOCAL-03993\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707824906333</updated>\n    </task>\n    <task id=\"LOCAL-03994\" summary=\"&#10;New Commit\">\n      <created>1707825140584</created>\n      <option name=\"number\" value=\"03994\" />\n      <option name=\"presentableId\" value=\"LOCAL-03994\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707825140584</updated>\n    </task>\n    <task id=\"LOCAL-03995\" summary=\"&#10;New Commit\">\n      <created>1707825296440</created>\n      <option name=\"number\" value=\"03995\" />\n      <option name=\"presentableId\" value=\"LOCAL-03995\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707825296440</updated>\n    </task>\n    <task id=\"LOCAL-03996\" summary=\"&#10;New Commit\">\n      <created>1707825393796</created>\n      <option name=\"number\" value=\"03996\" />\n      <option name=\"presentableId\" value=\"LOCAL-03996\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707825393796</updated>\n    </task>\n    <task id=\"LOCAL-03997\" summary=\"&#10;New Commit\">\n      <created>1707825630017</created>\n      <option name=\"number\" value=\"03997\" />\n      <option name=\"presentableId\" value=\"LOCAL-03997\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707825630017</updated>\n    </task>\n    <task id=\"LOCAL-03998\" summary=\"&#10;New Commit\">\n      <created>1707826652506</created>\n      <option name=\"number\" value=\"03998\" />\n      <option name=\"presentableId\" value=\"LOCAL-03998\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707826652506</updated>\n    </task>\n    <task id=\"LOCAL-03999\" summary=\"&#10;New Commit\">\n      <created>1707826705694</created>\n      <option name=\"number\" value=\"03999\" />\n      <option name=\"presentableId\" value=\"LOCAL-03999\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707826705694</updated>\n    </task>\n    <task id=\"LOCAL-04000\" summary=\"&#10;New Commit\">\n      <created>1707826748134</created>\n      <option name=\"number\" value=\"04000\" />\n      <option name=\"presentableId\" value=\"LOCAL-04000\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707826748134</updated>\n    </task>\n    <task id=\"LOCAL-04001\" summary=\"&#10;New Commit\">\n      <created>1707826792620</created>\n      <option name=\"number\" value=\"04001\" />\n      <option name=\"presentableId\" value=\"LOCAL-04001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707826792620</updated>\n    </task>\n    <task id=\"LOCAL-04002\" summary=\"&#10;New Commit\">\n      <created>1707827138054</created>\n      <option name=\"number\" value=\"04002\" />\n      <option name=\"presentableId\" value=\"LOCAL-04002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707827138054</updated>\n    </task>\n    <task id=\"LOCAL-04003\" summary=\"&#10;New Commit\">\n      <created>1707827165002</created>\n      <option name=\"number\" value=\"04003\" />\n      <option name=\"presentableId\" value=\"LOCAL-04003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707827165002</updated>\n    </task>\n    <task id=\"LOCAL-04004\" summary=\"&#10;New Commit\">\n      <created>1707827253795</created>\n      <option name=\"number\" value=\"04004\" />\n      <option name=\"presentableId\" value=\"LOCAL-04004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707827253795</updated>\n    </task>\n    <task id=\"LOCAL-04005\" summary=\"&#10;New Commit\">\n      <created>1707827360595</created>\n      <option name=\"number\" value=\"04005\" />\n      <option name=\"presentableId\" value=\"LOCAL-04005\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1707827360595</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"5139\" />\n    <servers />\n  </component>\n  <component name=\"Vcs.Log.History.Properties\">\n    <option name=\"COLUMN_ID_ORDER\">\n      <list>\n        <option value=\"Default.Root\" />\n        <option value=\"Default.Author\" />\n        <option value=\"Default.Date\" />\n        <option value=\"Default.Subject\" />\n        <option value=\"Space.CommitStatus\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Vcs.Log.Tabs.Properties\">\n    <option name=\"TAB_STATES\">\n      <map>\n        <entry key=\"MAIN\">\n          <value>\n            <State />\n          </value>\n        </entry>\n      </map>\n    </option>\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <option name=\"ADD_EXTERNAL_FILES_SILENTLY\" value=\"true\" />\n    <MESSAGE value=\"1st commit\" />\n    <MESSAGE value=\"2nd commit\" />\n    <MESSAGE value=\"3rd commit\" />\n    <MESSAGE value=\"Commit Feb 23 2023\" />\n    <MESSAGE value=\"no comment\" />\n    <MESSAGE value=\"Flowchart commit\" />\n    <MESSAGE value=\"Flowchart commi\" />\n    <MESSAGE value=\"REDCAP INPUT\" />\n    <MESSAGE value=\"&#10;New Commit\" />\n    <MESSAGE value=\"&#10;New Commitdgfdgf\" />\n    <MESSAGE value=\"New Commitdgfdgf\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"New Commitdgfdgf\" />\n  </component>\n  <component name=\"XDebuggerManager\">\n    <breakpoint-manager>\n      <breakpoints>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/download_with_session_ID.py</url>\n          <line>2</line>\n          <option name=\"timeStamp\" value=\"1\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/../biomarkerdbclass.py</url>\n          <line>76</line>\n          <option name=\"timeStamp\" value=\"2\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/download_with_session_ID.py</url>\n          <line>1888</line>\n          <option name=\"timeStamp\" value=\"3\" />\n        </line-breakpoint>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/download_with_session_ID.py</url>\n          <line>1865</line>\n          <option name=\"timeStamp\" value=\"4\" />\n        </line-breakpoint>\n      </breakpoints>\n    </breakpoint-manager>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 51d7187cf4437c7f93b9ba35f801fcadb7f1d19c)
+++ b/.idea/workspace.xml	(date 1761254533490)
@@ -4,7 +4,11 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="b7703539-bebb-46f9-9292-022aedf7fa67" name="Changes" comment="New Commitdgfdgf" />
+    <list default="true" id="b7703539-bebb-46f9-9292-022aedf7fa67" name="Changes" comment="New Commitdgfdgf">
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/download_ich_csvs.sh" beforeDir="false" afterPath="$PROJECT_DIR$/download_ich_csvs.sh" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/download_with_session_ID.py" beforeDir="false" afterPath="$PROJECT_DIR$/download_with_session_ID.py" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
@@ -482,7 +486,7 @@
       <option name="project" value="LOCAL" />
       <updated>1707827360595</updated>
     </task>
-    <option name="localTasksCounter" value="5139" />
+    <option name="localTasksCounter" value="5141" />
     <servers />
   </component>
   <component name="Vcs.Log.History.Properties">
